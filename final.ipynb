{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":140601,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":119089,"modelId":142340}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms, models\nfrom ultralytics import SAM\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nimport pandas as pd\nfrom PIL import Image\nimport os\nfrom sklearn.metrics import jaccard_score\nfrom transformers import Qwen2VLForConditionalGeneration, AutoProcessor\nimport warnings\nimport re\nfrom datetime import datetime\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Ensure GPU usage if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Helper functions\ndef iou(mask1, mask2):\n    return jaccard_score(mask1.flatten(), mask2.flatten())\n\ndef place_on_white_background(image):\n    white_bg = np.ones_like(image) * 255\n    mask = np.any(image != [0, 0, 0], axis=-1)\n    white_bg[mask] = image[mask]\n    return white_bg\n\ndef process_sam_results(results, model_name, iou_threshold=0.8, min_object_size=50):\n    original_image = results[0].orig_img\n    masks = results[0].masks.data.cpu().numpy()\n    \n    unique_objects = []\n    for i, mask in enumerate(masks):\n        is_unique = True\n        for existing_obj in unique_objects:\n            if iou(mask, existing_obj['mask']) > iou_threshold:\n                is_unique = False\n                break\n        \n        if is_unique:\n            object_mask = mask.astype(bool)\n            object_image = np.zeros_like(original_image)\n            object_image[object_mask] = original_image[object_mask]\n            \n            y, x = np.where(object_mask)\n            y1, y2, x1, x2 = y.min(), y.max(), x.min(), x.max()\n            \n            cropped_object = object_image[y1:y2, x1:x2]\n            \n            if cropped_object.shape[0] < min_object_size or cropped_object.shape[1] < min_object_size:\n                continue\n            \n            object_on_white = place_on_white_background(cropped_object)\n            \n            object_filename = f\"{model_name}object{len(unique_objects)}.png\"\n            cv2.imwrite(object_filename, cv2.cvtColor(object_on_white, cv2.COLOR_RGB2BGR))\n            \n            unique_objects.append({\n                'model': model_name,\n                'object_id': len(unique_objects),\n                'image': Image.fromarray(object_on_white),\n                'mask': mask,\n                'bbox': [x1, y1, x2, y2],\n                'filename': object_filename\n            })\n    \n    return unique_objects\n\ndef visualize_objects(objects, cols=5):\n    n = len(objects)\n    rows = (n + cols - 1) // cols\n    fig, axs = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n    axs = axs.flatten()\n    \n    for i, obj in enumerate(objects):\n        axs[i].imshow(obj['image'])\n        axs[i].axis('off')\n        axs[i].set_title(f\"Object {obj['object_id']}\")\n    \n    for i in range(n, len(axs)):\n        axs[i].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Load models\nsam_model = SAM(\"sam2_b.pt\")\n\npacked_unpacked_model = models.efficientnet_b0(pretrained=False)\npacked_unpacked_model.classifier[1] = nn.Linear(packed_unpacked_model.classifier[1].in_features, 2)\npacked_unpacked_model.load_state_dict(torch.load('/kaggle/input/models-loaded/keras/default/1/efficientnet_b0_packed_unpacked (2).pth', map_location=device))\npacked_unpacked_model.to(device)\npacked_unpacked_model.eval()\n\nfruit_veg_model = models.efficientnet_b0(pretrained=False)\nfruit_veg_model.classifier[1] = nn.Linear(fruit_veg_model.classifier[1].in_features, 54)\nfruit_veg_model.load_state_dict(torch.load('/kaggle/input/models-loaded/keras/default/1/efficientnet_b0_fruit_veg_1 (1).pth', map_location=device))\nfruit_veg_model.to(device)\nfruit_veg_model.eval()\n\nfresh_rotten_model = models.efficientnet_b0(pretrained=False)\nfresh_rotten_model.classifier[1] = nn.Linear(fresh_rotten_model.classifier[1].in_features, 2)\nfresh_rotten_model.load_state_dict(torch.load('/kaggle/input/models-loaded/keras/default/1/efficientnet_b0_fruit_veg (2).pth', map_location=device))\nfresh_rotten_model.to(device)\nfresh_rotten_model.eval()\n\nqwen_model = Qwen2VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2-VL-2B-Instruct\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\",\n)\nqwen_processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n\n# Define categories and transformations\npacked_unpacked_categories = ['Packed', 'Unpacked']\npacked_unpacked_idx_to_category = {idx: category for idx, category in enumerate(packed_unpacked_categories)}\n\nfruit_veg_categories = [\n    'Orange', 'Tamarillo', 'Lime', 'Pomegranate', 'Plum', 'Pineapple', 'Apple', 'Dates', 'Papaya', 'Guava',\n    'Beetroot', 'Pear', 'Strawberry', 'Blueberry', 'Lulo', 'Avacado', 'Lemon', 'Kaki', 'Peach', 'Grape',\n    'Banana', 'Cherry', 'Watermelon', 'Mango', 'Grapefruit', 'Broccoli', 'Capsicum', 'Radish', 'Tomato', 'Turnip',\n    'Ginger', 'Zucchini', 'Brinjal', 'Pumpkin', 'Bell Pepper', 'Carrot', 'New Mexico Green Chile', 'Eggplant',\n    'Baby Corn', 'Zucchini dark', 'Sweet corn', 'Cabbage', 'Bitter_Gourd', 'Cauliflower', 'Chile Pepper',\n    'Sweet Potato', 'Bean', 'Cucumber', 'Bottle Gourd', 'Garlic', 'Peas', 'Onion', 'Potato', 'Spinach'\n]\nfruit_veg_idx_to_category = {idx: category for idx, category in enumerate(fruit_veg_categories)}\n\nfresh_rotten_categories = ['Fresh', 'Rotten']\nfresh_rotten_idx_to_category = {idx: category for idx, category in enumerate(fresh_rotten_categories)}\n\npacked_categories = [\n    'Staples', 'Snacks & Beverages', 'Packaged Food', 'Personal & Baby Care',\n    'Household Care', 'Dairy & Eggs', 'Home & Kitchen'\n]\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ndef predict_image_class(model, image_path, idx_to_category):\n    try:\n        image = Image.open(image_path).convert('RGB')\n        image = transform(image).unsqueeze(0).to(device)\n\n        with torch.no_grad():\n            outputs = model(image)\n            _, predicted_label = torch.max(outputs, 1)\n\n        predicted_label = predicted_label.item()\n        predicted_category = idx_to_category[predicted_label]\n\n        return predicted_label, predicted_category\n    except Exception as e:\n        print(f\"Error predicting class for {image_path}: {str(e)}\")\n        return None, None\n\ndef get_product_info(image_path, question):\n    try:\n        image = Image.open(image_path)\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"image\"},\n                    {\"type\": \"text\", \"text\": question}\n                ]\n            }\n        ]\n        text_prompt = qwen_processor.apply_chat_template(messages, add_generation_prompt=True)\n        inputs = qwen_processor(text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\")\n        inputs = inputs.to(\"cuda\")\n        output_ids = qwen_model.generate(**inputs, max_new_tokens=1024)\n        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n        output_text = qwen_processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n        return output_text[0]\n    except Exception as e:\n        print(f\"Error getting product info for {image_path}: {str(e)}\")\n        return \"Error: Unable to process image\"\n\ndef is_valid_date(date_str):\n    date_formats = [\n        \"%d/%m/%Y\", \"%d-%m-%Y\", \"%Y/%m/%d\", \"%Y-%m-%d\", \n        \"%m/%d/%Y\", \"%m-%d-%Y\", \"%d %b %Y\", \"%d %B %Y\", \n        \"%d/%m/%y\", \"%d-%m-%y\", \"%Y%m%d\"\n    ]\n    for date_format in date_formats:\n        try:\n            datetime.strptime(date_str, date_format)\n            return True\n        except ValueError:\n            continue\n    return False\n\ndef process_single_object(image_path):\n    # Check if packed or unpacked\n    _, packed_unpacked_category = predict_image_class(packed_unpacked_model, image_path, packed_unpacked_idx_to_category)\n    \n    if packed_unpacked_category == 'Packed':\n        # Get product information using Qwen model\n        product_name = get_product_info(image_path, \"What is the name of the product? NOTE: JUST PROVIDE NAME AS THE ANSWER\")\n        expiry_date = get_product_info(image_path, \"What is the expiry date of the product? If not visible, say 'Not visible'\")\n        description = get_product_info(image_path, \"Provide a brief description of the product\")\n        category_info = get_product_info(image_path, \"Classify the product into one of these categories: Staples, Snacks & Beverages, Packaged Food, Personal & Baby Care, Household Care, Dairy & Eggs, Home & Kitchen\")\n        \n        return pd.DataFrame({\n            'name': [product_name],\n            'expiry_date': [expiry_date],\n            'description': [description],\n            'category': [category_info],\n            'type': ['Packed'],\n            'frequency': [1]\n        })\n    else:\n        # Classify fruit/vegetable\n        _, fruit_veg_category = predict_image_class(fruit_veg_model, image_path, fruit_veg_idx_to_category)\n        \n        # Determine if fresh or rotten\n        _, fresh_rotten_category = predict_image_class(fresh_rotten_model, image_path, fresh_rotten_idx_to_category)\n        \n        return pd.DataFrame({\n            'name': [fruit_veg_category],\n            'condition': [fresh_rotten_category],\n            'type': ['Unpacked'],\n            'frequency': [1],\n            'indepth_condition': ['To be determined'],  # Placeholder\n            'weight': ['To be determined']  # Placeholder\n        })\n\ndef process_multiple_objects(image_path, is_front):\n    # Perform segmentation\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    results = sam_model(img)\n    objects = process_sam_results(results, \"sam\")\n    \n    df_list = []\n    for obj in objects:\n        obj_image_path = obj['filename']\n        obj_df = process_single_object(obj_image_path)\n        obj_df['bbox'] = [obj['bbox']]\n        obj_df['side'] = ['Front' if is_front else 'Back']\n        df_list.append(obj_df)\n    \n    return pd.concat(df_list, ignore_index=True)\n\ndef main():\n    image_type = input(\"Is this a single object image or multiple objects image? (single/multiple): \").lower()\n    \n    if image_type == 'single':\n        front_image_path = input(\"Enter the path to the front image: \")\n        back_image_path = input(\"Enter the path to the back image: \")\n        \n        front_df = process_single_object(front_image_path)\n        front_df['side'] = 'Front'\n        back_df = process_single_object(back_image_path)\n        back_df['side'] = 'Back'\n        \n        combined_df = pd.concat([front_df, back_df], ignore_index=True)\n        \n    elif image_type == 'multiple':\n        front_image_path = input(\"Enter the path to the front image: \")\n        back_image_path = input(\"Enter the path to the back image: \")\n        \n        front_df = process_multiple_objects(front_image_path, is_front=True)\n        back_df = process_multiple_objects(back_image_path, is_front=False)\n        \n        combined_df = pd.concat([front_df, back_df], ignore_index=True)\n        \n        # Instead of grouping by 'bbox', we'll create a unique identifier\n        combined_df['object_id'] = range(len(combined_df))\n        \n        # Merge front and back information based on object overlap\n        # This is a simplified approach and may need refinement\n        merged_df = combined_df.groupby('object_id').first().reset_index()\n        \n    else:\n        print(\"Invalid input. Please choose 'single' or 'multiple'.\")\n        return\n    \n    # Create separate DataFrames for packed and unpacked items\n    packed_df = combined_df[combined_df['type'] == 'Packed']\n    unpacked_df = combined_df[combined_df['type'] == 'Unpacked']\n    \n    # Save results\n    combined_df.to_csv('combined_items.csv', index=False)\n    packed_df.to_csv('packed_items.csv', index=False)\n    unpacked_df.to_csv('unpacked_items.csv', index=False)\n    \n    print(\"Processing complete. Results saved to CSV files.\")\n    print(\"\\nCombined items:\")\n    print(combined_df)\n    print(\"\\nPacked items:\")\n    print(packed_df)\n    print(\"\\nUnpacked items:\")\n    print(unpacked_df)\n\n    \nif __name__ == \"__main__\":\n    main()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"packed_df=pd.read_csv('/kaggle/working/packed_items.csv')\npacked_df.head()\n\nunpacked_df=pd.read_csv('/kaggle/working/unpacked_items.csv')\nunpacked_df\n\ncombined_df=pd.read_csv('/kaggle/working/combined_items.csv')\ncombined_df","metadata":{},"execution_count":null,"outputs":[]}]}