{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9574587,"sourceType":"datasetVersion","datasetId":5836668},{"sourceId":9590834,"sourceType":"datasetVersion","datasetId":5849488}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        continue\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading and Analysis of DataFrame","metadata":{}},{"cell_type":"code","source":"import os\nimport csv\n\ndef categorize_image(directory):\n    if directory.startswith('S_') or directory.endswith('Bad') or directory.startswith('Rotten') or directory=='Old' or directory=='Dried' or directory=='Damaged' or directory=='Formalin-mixed' in directory:\n        return 'Rotten'\n    else:\n        return 'Fresh'\n\ndef create_csv(root_dir, output_file):\n    with open(output_file, 'w', newline='') as csvfile:\n        fieldnames = ['Image Directory', 'Main Directory', 'Category', 'Sub Category', 'Sub Sub Category']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n\n        for main_dir in ['Fruits', 'Vegetables']:\n            main_path = os.path.join(root_dir, 'Detection', main_dir)\n            \n            for category in os.listdir(main_path):\n                category_path = os.path.join(main_path, category)\n                \n                if os.path.isdir(category_path):\n                    for sub_category in os.listdir(category_path):\n                        sub_category_path = os.path.join(category_path, sub_category)\n                        \n                        if os.path.isdir(sub_category_path):\n                            sub_sub_category = categorize_image(sub_category)\n                            \n                            for image in os.listdir(sub_category_path):\n                                if image.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n                                    image_path = os.path.join(sub_category_path, image)\n                                    writer.writerow({\n                                        'Image Directory': image_path,\n                                        'Main Directory': main_dir,\n                                        'Category': category,\n                                        'Sub Category': sub_category,\n                                        'Sub Sub Category': sub_sub_category\n                                    })\n\n# Usage\nroot_directory = '/kaggle/input/woking-dataset'\noutput_csv = 'fruit_vegetable_dataset.csv'\ncreate_csv(root_directory, output_csv)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndf=pd.read_csv(output_csv)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['Category']=='1. Bell Pepper']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['Category']=='Spinach']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['Sub Sub Category']=='Fresh']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['Sub Sub Category']!='Fresh']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tried Multiple things (no need to run)","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# Ensure GPU usage if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load the dataset\ndf = pd.read_csv('fruit_vegetable_dataset.csv')\n\n# Map categories to numerical labels\ncategories = df['Category'].unique()\ncategory_to_idx = {category: idx for idx, category in enumerate(categories)}\ndf['Label'] = df['Category'].map(category_to_idx)\n\n# Split the dataset into train, validation, and test sets\ntrain_val, test_df = train_test_split(df, test_size=0.2, stratify=df['Label'])\ntrain_df, val_df = train_test_split(train_val, test_size=0.2, stratify=train_val['Label'])\n\n# Custom Dataset class for loading images and labels\nclass FruitVegDataset(Dataset):\n    def __init__(self, dataframe, transform=None):\n        self.dataframe = dataframe.reset_index(drop=True)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        img_path = self.dataframe.iloc[idx]['Image Directory']\n        image = Image.open(img_path).convert('RGB')\n        label = self.dataframe.iloc[idx]['Label']\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n# Define transformations for the training and validation sets\ntransform_train = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntransform_val_test = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Create datasets and dataloaders\ntrain_dataset = FruitVegDataset(train_df, transform=transform_train)\nval_dataset = FruitVegDataset(val_df, transform=transform_val_test)\ntest_dataset = FruitVegDataset(test_df, transform=transform_val_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32)\ntest_loader = DataLoader(test_dataset, batch_size=32)\n\n# Function to train and evaluate a model using ResNet18 (similar approach can be applied to ResNet50)\ndef train_and_evaluate_resnet():\n    # Load pre-trained ResNet18 model and modify the final layer for our dataset\n    model = models.resnet18(pretrained=True)\n    model.fc = nn.Linear(model.fc.in_features, len(categories))\n    \n    model = model.to(device)\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n    num_epochs = 10\n    best_accuracy = 0\n\n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        running_loss = 0.0\n        \n        for images, labels in tqdm(train_loader, desc=f'Training ResNet18 Epoch {epoch+1}/{num_epochs}'):\n            images, labels = images.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item() * images.size(0)\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n\n        # Validation phase\n        model.eval()\n        corrects = 0\n        \n        with torch.no_grad():\n            for images, labels in tqdm(val_loader, desc=f'Validating ResNet18'):\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                _, preds = torch.max(outputs, 1)\n                corrects += torch.sum(preds == labels).item()\n\n        epoch_accuracy = corrects / len(val_loader.dataset)\n\n        print(f'ResNet18 - Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n\n        # Save the best model based on validation accuracy\n        if epoch_accuracy > best_accuracy:\n            best_accuracy = epoch_accuracy\n            best_model_wts = model.state_dict()\n\n    # Load best model weights and evaluate on test set\n    model.load_state_dict(best_model_wts)\n    \n    # Test phase\n    model.eval()\n    test_corrects = 0\n    \n    with torch.no_grad():\n        for images, labels in tqdm(test_loader, desc=f'Testing ResNet18'):\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n            test_corrects += torch.sum(preds == labels).item()\n\n    test_accuracy = test_corrects / len(test_loader.dataset)\n    print(f'ResNet18 - Test Accuracy: {test_accuracy:.4f}')\n\n    # Save the trained model\n    torch.save(model.state_dict(), 'resnet18_fruit_veg.pth')\n    print(\"Model saved as 'resnet18_fruit_veg.pth'\")\n\n# Train and evaluate the ResNet18 model\ntrain_and_evaluate_resnet()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch,gc\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset, DatasetDict\nfrom transformers import ViTForImageClassification, ViTImageProcessor, TrainingArguments, Trainer\nfrom transformers import DefaultDataCollator\nimport torch\nfrom PIL import Image\nimport gc\n\n# Ensure GPU usage if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load the dataset\n#df = pd.read_csv('/kaggle/input/your-dataset-name/fruit_vegetable_dataset.csv')\n\n# Map categories to numerical labels\ncategories = df['Category'].unique()\ncategory_to_idx = {category: idx for idx, category in enumerate(categories)}\ndf['Label'] = df['Category'].map(category_to_idx)\n\n# Split the dataset into train, validation, and test sets\ntrain_val_df, test_df = train_test_split(df, test_size=0.2, stratify=df['Label'])\ntrain_df, val_df = train_test_split(train_val_df, test_size=0.2, stratify=train_val_df['Label'])\n\n# Convert DataFrame to Hugging Face Dataset format\ndef df_to_dataset(dataframe):\n    return Dataset.from_pandas(dataframe[['Image Directory', 'Label']])\n\ntrain_dataset = df_to_dataset(train_df)\nval_dataset = df_to_dataset(val_df)\ntest_dataset = df_to_dataset(test_df)\n\ndataset_dict = DatasetDict({\n    'train': train_dataset,\n    'validation': val_dataset,\n    'test': test_dataset\n})\n\n# Load ViT Image Processor\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224', device=device)\n\n# Define a function to preprocess images and manage memory\ndef preprocess_images(examples):\n    images = []\n    for image_path in examples['Image Directory']:\n        image = Image.open(image_path).convert(\"RGB\")\n        images.append(image)\n    \n    # Apply processing and convert to PyTorch tensors\n    processed_images = processor(images=images, return_tensors=\"pt\")\n\n    # Clear memory after each batch\n    del images\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    # Return processed images and labels\n    return {\n        'pixel_values': processed_images['pixel_values'],\n        'labels': examples['Label']\n    }\n\n# Apply preprocessing to datasets using batched mapping for efficiency\ndataset_dict = dataset_dict.map(preprocess_images, batched=True, batch_size=32)\n\n# Load pre-trained ViT model for classification\nmodel = ViTForImageClassification.from_pretrained(\n    'google/vit-base-patch16-224',\n    num_labels=len(categories),\n    id2label={i: label for i, label in enumerate(categories)},\n    label2id={label: i for i, label in enumerate(categories)}\n).to(device)\n\n# Set training arguments, including smaller batch size to manage memory usage\ntraining_args = TrainingArguments(\n    output_dir=\"./vit-model\",\n    per_device_train_batch_size=4,  # Reduce batch size for lower memory consumption\n    per_device_eval_batch_size=4,   # Reduce eval batch size as well\n    num_train_epochs=5,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    load_best_model_at_end=True\n)\n\n# Define data collator\ndata_collator = DefaultDataCollator()\n\n# Define the metrics computation function\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = torch.argmax(logits, dim=-1)\n    accuracy = (predictions == labels).float().mean()\n    return {\"accuracy\": accuracy.item()}\n\n# Initialize Trainer with memory management considerations\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset_dict['train'],\n    eval_dataset=dataset_dict['validation'],\n    tokenizer=processor,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\n# Start training with GPU memory management in place\ntrainer.train()\n\n# Evaluate the model on the test dataset with memory management\nmetrics = trainer.evaluate(dataset_dict['test'])\nprint(metrics)\n\n# Save the trained model\ntrainer.save_model(\"vit_fruit_veg_classifier\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset, DatasetDict\nfrom transformers import ViTForImageClassification, ViTImageProcessor, TrainingArguments, Trainer\nfrom transformers import DefaultDataCollator\nimport torch\nfrom PIL import Image\nimport gc\n\n# Ensure GPU usage if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load the dataset\ndf = pd.read_csv('fruit_vegetable_dataset.csv')\n\n# Map categories to numerical labels\ncategories = df['Category'].unique()\ncategory_to_idx = {category: idx for idx, category in enumerate(categories)}\ndf['Label'] = df['Category'].map(category_to_idx)\n\n# Split the dataset into train, validation, and test sets\ntrain_val_df, test_df = train_test_split(df, test_size=0.2, stratify=df['Label'])\ntrain_df, val_df = train_test_split(train_val_df, test_size=0.2, stratify=train_val_df['Label'])\n\n# Convert DataFrame to Hugging Face Dataset format\ndef df_to_dataset(dataframe):\n    return Dataset.from_pandas(dataframe[['Image Directory', 'Label']])\n\ntrain_dataset = df_to_dataset(train_df)\nval_dataset = df_to_dataset(val_df)\ntest_dataset = df_to_dataset(test_df)\n\ndataset_dict = DatasetDict({\n    'train': train_dataset,\n    'validation': val_dataset,\n    'test': test_dataset\n})\n\n# Load ViT Image Processor\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n\n# Define a function to preprocess images in chunks and manage memory\ndef preprocess_images_in_chunks(dataset, chunk_size=1000):\n    processed_batches = []\n    for i in range(0, len(dataset), chunk_size):\n        batch = dataset[i:i+chunk_size]\n        images = [Image.open(image_path).convert(\"RGB\") for image_path in batch['Image Directory']]\n        processed_images = processor(images=images, return_tensors=\"pt\")\n        \n        # Dispose of images and clear memory after processing each chunk\n        del images\n        gc.collect()\n        torch.cuda.empty_cache()\n        \n        processed_batches.append(processed_images)\n    \n    return processed_batches\n\n# Preprocess datasets in chunks to handle large data efficiently\ntrain_processed_batches = preprocess_images_in_chunks(dataset_dict['train'])\nval_processed_batches = preprocess_images_in_chunks(dataset_dict['validation'])\ntest_processed_batches = preprocess_images_in_chunks(dataset_dict['test'])\n\n# Load pre-trained ViT model and modify for our task\nmodel = ViTForImageClassification.from_pretrained(\n    'google/vit-base-patch16-224',\n    num_labels=len(categories),\n    id2label={i: label for i, label in enumerate(categories)},\n    label2id={label: i for i, label in enumerate(categories)}\n).to(device)\n\n# Define training arguments with lower batch size if needed to fit into memory\ntraining_args = TrainingArguments(\n    output_dir=\"./vit-model\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=5,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    load_best_model_at_end=True,\n)\n\n# Define data collator\ndata_collator = DefaultDataCollator()\n\n# Define metrics function\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = torch.argmax(logits, dim=-1)\n    accuracy = (predictions == labels).float().mean()\n    return {\"accuracy\": accuracy.item()}\n\n# Initialize Trainer with memory management considerations\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_processed_batches,\n    eval_dataset=val_processed_batches,\n    tokenizer=processor,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\n# Train and evaluate the model with memory management steps included in the training loop\ntrainer.train()\n\n# Evaluate on the test set with memory management considerations\nmetrics = trainer.evaluate(test_processed_batches)\nprint(metrics)\n\n# Save the trained model with consideration for memory management\ntrainer.save_model(\"vit_fruit_veg_classifier\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Category , Sub Category and Sub Sub Category Analysis ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('fruit_vegetable_dataset.csv')\n\n# Display the count of each category\ncategory_counts = df['Category'].value_counts()\nprint(\"Category Counts:\")\nprint(category_counts)\n\n# Optionally, display counts for subcategories and sub-subcategories as well\nsub_category_counts = df['Sub Category'].value_counts()\nprint(\"\\nSub Category Counts:\")\nprint(sub_category_counts)\n\nsub_sub_category_counts = df['Sub Sub Category'].value_counts()\nprint(\"\\nSub Sub Category Counts:\")\nprint(sub_sub_category_counts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# For downloading it to System and also tried annotating and Equalizing data (Tried but not used) (No need to run)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.utils import resample\nfrom torchvision import transforms\nfrom PIL import Image\nimport os\n\n# Load the dataset\ndf = pd.read_csv('fruit_vegetable_dataset.csv')\n\n# Define the target number of images per category\nTARGET_IMAGES_PER_CATEGORY = 3000\n\n# Define image augmentation transformations for oversampling\naugmentation_transforms = transforms.Compose([\n    transforms.RandomRotation(degrees=15),\n    transforms.RandomHorizontalFlip(),\n])\n\ndef augment_image(image_path, output_dir, augment_count):\n    \"\"\"Augment and save images.\"\"\"\n    image = Image.open(image_path).convert('RGB')\n    for i in range(augment_count):\n        augmented_image = augmentation_transforms(image)\n        base_name = os.path.basename(image_path)\n        name, ext = os.path.splitext(base_name)\n        augmented_image_path = os.path.join(output_dir, f\"{name}_aug_{i}{ext}\")\n        augmented_image.save(augmented_image_path)\n        yield augmented_image_path\n\ndef balance_dataset(df):\n    balanced_data = []\n\n    # Create a directory for augmented images if it doesn't exist\n    aug_dir = 'augmented_images'\n    os.makedirs(aug_dir, exist_ok=True)\n\n    # Group by Category, Sub Category, and Sub Sub Category\n    for (category, sub_category, sub_sub_category), group in df.groupby(['Category', 'Sub Category', 'Sub Sub Category']):\n        current_count = len(group)\n        \n        if current_count < TARGET_IMAGES_PER_CATEGORY:\n            # Oversample by augmenting images\n            needed_images = TARGET_IMAGES_PER_CATEGORY - current_count\n            augment_per_image = needed_images // current_count + 1\n            \n            for _, row in group.iterrows():\n                image_path = row['Image Directory']\n                augmented_paths = list(augment_image(image_path, aug_dir, augment_per_image))\n                augmented_rows = [row.copy() for _ in augmented_paths]\n                \n                for aug_row, aug_path in zip(augmented_rows, augmented_paths):\n                    aug_row['Image Directory'] = aug_path\n                \n                balanced_data.extend(augmented_rows)\n            \n            # Add original data to balanced data\n            balanced_data.extend(group.to_dict('records'))\n        \n        elif current_count > TARGET_IMAGES_PER_CATEGORY:\n            # Undersample by taking a random sample of the group\n            sampled_group = group.sample(n=TARGET_IMAGES_PER_CATEGORY, random_state=42)\n            balanced_data.extend(sampled_group.to_dict('records'))\n        \n        else:\n            # If already at target size, add directly\n            balanced_data.extend(group.to_dict('records'))\n    \n    return pd.DataFrame(balanced_data)\n\n# Balance the dataset\nbalanced_df = balance_dataset(df)\n\n# Check the distribution after balancing\nprint(balanced_df['Category'].value_counts())\nprint(balanced_df['Sub Category'].value_counts())\nprint(balanced_df['Sub Sub Category'].value_counts())\n\n# Save the balanced dataset to a new CSV file (optional)\nbalanced_df.to_csv('balanced_fruit_vegetable_dataset.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.utils import resample\nimport albumentations as A\nimport cv2\nimport os\nfrom tqdm import tqdm\nimport shutil\n\n# Load the dataset\ndf = pd.read_csv('fruit_vegetable_dataset.csv')\n\n# Define the target number of images per category\nTARGET_IMAGES_PER_CATEGORY = 3000\n\n# Define image augmentation transformations using Albumentations\naugmentation_transforms = A.Compose([\n    A.RandomRotate90(p=0.5),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n])\n\ndef augment_image(image_path, output_dir, augment_count):\n    \"\"\"Augment and save images.\"\"\"\n    image = cv2.imread(image_path)\n    for i in range(augment_count):\n        augmented = augmentation_transforms(image=image)\n        augmented_image = augmented['image']\n        base_name = os.path.basename(image_path)\n        name, ext = os.path.splitext(base_name)\n        augmented_image_path = os.path.join(output_dir, f\"{name}_aug_{i}{ext}\")\n        cv2.imwrite(augmented_image_path, augmented_image)\n        yield augmented_image_path\n\ndef balance_dataset(df):\n    balanced_data = []\n\n    # Create a directory for augmented images if it doesn't exist\n    aug_dir = 'augmented_images'\n    os.makedirs(aug_dir, exist_ok=True)\n\n    # Group by Category, Sub Category, and Sub Sub Category\n    for (category, sub_category, sub_sub_category), group in df.groupby(['Category', 'Sub Category', 'Sub Sub Category']):\n        current_count = len(group)\n        \n        if current_count < TARGET_IMAGES_PER_CATEGORY:\n            # Oversample by augmenting images\n            needed_images = TARGET_IMAGES_PER_CATEGORY - current_count\n            augment_per_image = needed_images // current_count + 1\n            \n            for _, row in group.iterrows():\n                image_path = row['Image Directory']\n                augmented_paths = list(augment_image(image_path, aug_dir, augment_per_image))\n                augmented_rows = [row.copy() for _ in augmented_paths]\n                \n                for aug_row, aug_path in zip(augmented_rows, augmented_paths):\n                    aug_row['Image Directory'] = aug_path\n                \n                balanced_data.extend(augmented_rows)\n            \n            # Add original data to balanced data\n            balanced_data.extend(group.to_dict('records'))\n        \n        elif current_count > TARGET_IMAGES_PER_CATEGORY:\n            # Undersample by taking a random sample of the group\n            sampled_group = group.sample(n=TARGET_IMAGES_PER_CATEGORY, random_state=42)\n            balanced_data.extend(sampled_group.to_dict('records'))\n        \n        else:\n            # If already at target size, add directly\n            balanced_data.extend(group.to_dict('records'))\n    \n    return pd.DataFrame(balanced_data)\n\n# Balance the dataset\nbalanced_df = balance_dataset(df)\n\n# Check the distribution after balancing\nprint(balanced_df['Category'].value_counts())\nprint(balanced_df['Sub Category'].value_counts())\nprint(balanced_df['Sub Sub Category'].value_counts())\n\n# Save the balanced dataset to a new CSV file (optional)\nbalanced_df.to_csv('balanced_fruit_vegetable_dataset.csv', index=False)\n\n# Create a zip file of the balanced dataset images\nzip_dir = 'balanced_dataset_zip'\nos.makedirs(zip_dir, exist_ok=True)\n\nfor _, row in tqdm(balanced_df.iterrows(), total=len(balanced_df)):\n    src_path = row['Image Directory']\n    dst_path = os.path.join(zip_dir, os.path.basename(src_path))\n    shutil.copy(src_path, dst_path)\n\nshutil.make_archive('balanced_fruit_vegetable_dataset', 'zip', zip_dir)\n\nprint(\"Zipped dataset created: balanced_fruit_vegetable_dataset.zip\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport albumentations as A\nimport cv2\nimport os\nfrom tqdm import tqdm\n\n# Load the dataset\ndf = pd.read_csv('fruit_vegetable_dataset.csv')\n\n# Define the target number of images per category\nTARGET_IMAGES_PER_CATEGORY = 3000\n\n# Define image augmentation transformations using Albumentations\naugmentation_transforms = A.Compose([\n    A.RandomRotate90(p=0.5),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n])\n\ndef augment_image(image_path, output_dir, augment_count):\n    \"\"\"Augment and save images.\"\"\"\n    image = cv2.imread(image_path)\n    for i in range(augment_count):\n        augmented = augmentation_transforms(image=image)\n        augmented_image = augmented['image']\n        base_name = os.path.basename(image_path)\n        name, ext = os.path.splitext(base_name)\n        augmented_image_path = os.path.join(output_dir, f\"{name}_aug_{i}{ext}\")\n        cv2.imwrite(augmented_image_path, augmented_image)\n        yield augmented_image_path\n\ndef balance_dataset(df):\n    balanced_data = []\n\n    # Create a directory for augmented images if it doesn't exist\n    aug_dir = 'augmented_images'\n    os.makedirs(aug_dir, exist_ok=True)\n\n    # Group by Category and Sub Category\n    for (category, sub_category), group in df.groupby(['Category', 'Sub Category']):\n        current_count = len(group)\n        \n        print(f\"Processing {category} - {sub_category}: {current_count} images\")\n\n        if current_count < TARGET_IMAGES_PER_CATEGORY:\n            # Oversample by augmenting images\n            needed_images = TARGET_IMAGES_PER_CATEGORY - current_count\n            augment_per_image = needed_images // current_count + 1\n            \n            for _, row in tqdm(group.iterrows(), total=current_count, desc=f\"Augmenting {category} - {sub_category}\"):\n                image_path = row['Image Directory']\n                augmented_paths = list(augment_image(image_path, aug_dir, augment_per_image))\n                augmented_rows = [row.copy() for _ in augmented_paths]\n                \n                for aug_row, aug_path in zip(augmented_rows, augmented_paths):\n                    aug_row['Image Directory'] = aug_path\n                \n                balanced_data.extend(augmented_rows)\n            \n            # Add original data to balanced data\n            balanced_data.extend(group.to_dict('records'))\n        \n        elif current_count > TARGET_IMAGES_PER_CATEGORY:\n            # Undersample by taking a random sample of the group\n            sampled_group = group.sample(n=TARGET_IMAGES_PER_CATEGORY, random_state=42)\n            balanced_data.extend(sampled_group.to_dict('records'))\n        \n        else:\n            # If already at target size, add directly\n            balanced_data.extend(group.to_dict('records'))\n    \n    return pd.DataFrame(balanced_data)\n\n# Balance the dataset\nbalanced_df = balance_dataset(df)\n\n# Check the distribution after balancing\nprint(balanced_df['Category'].value_counts())\nprint(balanced_df['Sub Category'].value_counts())\n\n# Save the balanced dataset to a new CSV file\nbalanced_df.to_csv('balanced_fruit_vegetable_dataset.csv', index=False)\n\n# Organize images into a structured directory (optional)\noutput_dir = 'balanced_dataset_images'\nos.makedirs(output_dir, exist_ok=True)\n\nfor _, row in balanced_df.iterrows():\n    category_dir = os.path.join(output_dir, row['Category'])\n    os.makedirs(category_dir, exist_ok=True)\n    \n    src_path = row['Image Directory']\n    dst_path = os.path.join(category_dir, os.path.basename(src_path))\n    \n    if not os.path.exists(dst_path):\n        os.rename(src_path, dst_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport albumentations as A\nimport cv2\nimport os\nfrom tqdm import tqdm\nfrom sklearn.utils import resample\nimport shutil\nimport zipfile\n\n# Load the dataset\ndf = pd.read_csv('fruit_vegetable_dataset.csv')\n\n# Define the target number of images per category\nTARGET_IMAGES_PER_CATEGORY = 3000\n\n# Define image augmentation transformations using Albumentations\naugmentation_transforms = A.Compose([\n    A.RandomRotate90(p=0.5),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n])\n\ndef augment_image(image_path, output_dir, augment_count):\n    \"\"\"Augment and save images.\"\"\"\n    image = cv2.imread(image_path)\n    for i in range(augment_count):\n        augmented = augmentation_transforms(image=image)\n        augmented_image = augmented['image']\n        base_name = os.path.basename(image_path)\n        name, ext = os.path.splitext(base_name)\n        augmented_image_path = os.path.join(output_dir, f\"{name}_aug_{i}{ext}\")\n        cv2.imwrite(augmented_image_path, augmented_image)\n        yield augmented_image_path\n\ndef balance_dataset(df):\n    balanced_data = []\n\n    # Create a directory for the balanced dataset\n    balanced_dir = 'balanced_dataset'\n    os.makedirs(balanced_dir, exist_ok=True)\n\n    # Group by Category, Sub Category, and Sub Sub Category\n    for (category, sub_category, sub_sub_category), group in df.groupby(['Category', 'Sub Category', 'Sub Sub Category']):\n        current_count = len(group)\n        \n        # Create subdirectory for each category/subcategory/sub-subcategory\n        category_dir = os.path.join(balanced_dir, category, sub_category, sub_sub_category)\n        os.makedirs(category_dir, exist_ok=True)\n\n        if current_count < TARGET_IMAGES_PER_CATEGORY:\n            # Oversample by augmenting images\n            needed_images = TARGET_IMAGES_PER_CATEGORY - current_count\n            augment_per_image = needed_images // current_count + 1\n            \n            for _, row in group.iterrows():\n                image_path = row['Image Directory']\n                augmented_paths = list(augment_image(image_path, category_dir, augment_per_image))\n                augmented_rows = [row.copy() for _ in augmented_paths]\n                \n                for aug_row, aug_path in zip(augmented_rows, augmented_paths):\n                    aug_row['Image Directory'] = aug_path\n                \n                balanced_data.extend(augmented_rows)\n            \n            # Add original data to balanced data\n            balanced_data.extend(group.to_dict('records'))\n        \n        elif current_count > TARGET_IMAGES_PER_CATEGORY:\n            # Undersample by taking a random sample of the group\n            sampled_group = group.sample(n=TARGET_IMAGES_PER_CATEGORY, random_state=42)\n            for _, row in sampled_group.iterrows():\n                image_path = row['Image Directory']\n                shutil.copy(image_path, category_dir)\n            balanced_data.extend(sampled_group.to_dict('records'))\n        \n        else:\n            # If already at target size, add directly and copy files\n            for _, row in group.iterrows():\n                image_path = row['Image Directory']\n                shutil.copy(image_path, category_dir)\n            balanced_data.extend(group.to_dict('records'))\n    \n    return pd.DataFrame(balanced_data)\n\n# Balance the dataset\nbalanced_df = balance_dataset(df)\n\n# Check the distribution after balancing\nprint(balanced_df['Category'].value_counts())\nprint(balanced_df['Sub Category'].value_counts())\nprint(balanced_df['Sub Sub Category'].value_counts())\n\n# Save the balanced dataset to a new CSV file\nbalanced_csv_path = 'balanced_fruit_vegetable_dataset.csv'\nbalanced_df.to_csv(balanced_csv_path, index=False)\n\n# Zip the balanced dataset directory\nzip_filename = 'balanced_dataset.zip'\nwith zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n    for root, dirs, files in os.walk(balanced_dir):\n        for file in files:\n            file_path = os.path.join(root, file)\n            zipf.write(file_path, os.path.relpath(file_path, balanced_dir))\n\nprint(f\"Balanced dataset and CSV saved. Zip file created: {zip_filename}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport albumentations as A\nimport cv2\nimport os\nfrom tqdm import tqdm\nimport shutil\nimport zipfile\n\n# Load the dataset\ndf = pd.read_csv('fruit_vegetable_dataset.csv')\n\n# Define the target number of images per category\nTARGET_IMAGES_PER_CATEGORY = 3000\n\n# Define image augmentation transformations using Albumentations\naugmentation_transforms = A.Compose([\n    A.RandomRotate90(p=0.5),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n])\n\ndef augment_image(image_path, output_dir, augment_count):\n    \"\"\"Augment and save images.\"\"\"\n    image = cv2.imread(image_path)\n    for i in range(augment_count):\n        augmented = augmentation_transforms(image=image)\n        augmented_image = augmented['image']\n        base_name = os.path.basename(image_path)\n        name, ext = os.path.splitext(base_name)\n        augmented_image_path = os.path.join(output_dir, f\"{name}_aug_{i}{ext}\")\n        cv2.imwrite(augmented_image_path, augmented_image)\n        yield augmented_image_path\n\ndef balance_dataset(df):\n    balanced_data = []\n\n    # Create a directory for the balanced dataset\n    balanced_dir = 'balanced_dataset'\n    os.makedirs(balanced_dir, exist_ok=True)\n\n    # Group by Category and Sub Category\n    for category, group in df.groupby('Category'):\n        current_count = len(group)\n        \n        # Create subdirectory for each category\n        category_dir = os.path.join(balanced_dir, category)\n        os.makedirs(category_dir, exist_ok=True)\n\n        if current_count < TARGET_IMAGES_PER_CATEGORY:\n            # Oversample by augmenting images\n            needed_images = TARGET_IMAGES_PER_CATEGORY - current_count\n            augment_per_image = needed_images // current_count + 1\n            \n            for _, row in group.iterrows():\n                image_path = row['Image Directory']\n                augmented_paths = list(augment_image(image_path, category_dir, augment_per_image))\n                augmented_rows = [row.copy() for _ in augmented_paths]\n                \n                for aug_row, aug_path in zip(augmented_rows, augmented_paths):\n                    aug_row['Image Directory'] = aug_path\n                \n                balanced_data.extend(augmented_rows)\n            \n            # Add original data to balanced data\n            balanced_data.extend(group.to_dict('records'))\n        \n        elif current_count > TARGET_IMAGES_PER_CATEGORY:\n            # Undersample by taking a random sample of the group\n            sampled_group = group.sample(n=TARGET_IMAGES_PER_CATEGORY, random_state=42)\n            for _, row in sampled_group.iterrows():\n                image_path = row['Image Directory']\n                shutil.copy(image_path, category_dir)\n            balanced_data.extend(sampled_group.to_dict('records'))\n        \n        else:\n            # If already at target size, add directly and copy files\n            for _, row in group.iterrows():\n                image_path = row['Image Directory']\n                shutil.copy(image_path, category_dir)\n            balanced_data.extend(group.to_dict('records'))\n    \n    return pd.DataFrame(balanced_data)\n\n# Balance the dataset\nbalanced_df = balance_dataset(df)\n\n# Check the distribution after balancing\nprint(balanced_df['Category'].value_counts())\n\n# Save the balanced dataset to a new CSV file\nbalanced_csv_path = 'balanced_fruit_vegetable_dataset.csv'\nbalanced_df.to_csv(balanced_csv_path, index=False)\n\n# Zip the balanced dataset directory\nzip_filename = 'balanced_dataset.zip'\nwith zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n    for root, dirs, files in os.walk('balanced_dataset'):\n        for file in files:\n            file_path = os.path.join(root, file)\n            zipf.write(file_path, os.path.relpath(file_path, 'balanced_dataset'))\n\nprint(f\"Balanced dataset and CSV saved. Zip file created: {zip_filename}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Sample DataFrame (replace this with your actual DataFrame)\n\n# Function to create the balanced DataFrame\ndef create_balanced_df(df, target_count=3000):\n    balanced_df = []\n\n    for category, group in df.groupby('Category'):\n        subcategory_groups = group.groupby('Sub Category')\n        num_subcategories = len(subcategory_groups)\n\n        # Calculate how many images to take from each subcategory\n        images_per_subcategory = target_count // num_subcategories\n\n        for sub_category, sub_group in subcategory_groups:\n            current_count = len(sub_group)\n\n            if current_count < images_per_subcategory:\n                # Duplicate the group until we reach the required count\n                duplicates_needed = images_per_subcategory // current_count\n                remainder_needed = images_per_subcategory % current_count\n                \n                # Create the balanced group\n                balanced_group = pd.concat(\n                    [sub_group] * duplicates_needed +\n                    [sub_group.sample(remainder_needed, replace=True)]\n                )\n            else:\n                # Sample images if we have more than needed\n                balanced_group = sub_group.sample(images_per_subcategory, random_state=1)\n\n            balanced_df.append(balanced_group)\n\n    return pd.concat(balanced_df, ignore_index=True)\n\n# Create the new balanced DataFrame\nnew_df = create_balanced_df(df, target_count=3000)\n\n# Output the new DataFrame\nprint(new_df)\nprint(f\"New DataFrame shape: {new_df.shape}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n\n# Display the count of each category\ncategory_counts = new_df['Category'].value_counts()\nprint(\"Category Counts:\")\nprint(category_counts)\n\n# Optionally, display counts for subcategories and sub-subcategories as well\nsub_category_counts = new_df['Sub Category'].value_counts()\nprint(\"\\nSub Category Counts:\")\nprint(sub_category_counts)\n\nsub_sub_category_counts = new_df['Sub Sub Category'].value_counts()\nprint(\"\\nSub Sub Category Counts:\")\nprint(sub_sub_category_counts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Efficient Net For Category ","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# Ensure GPU usage if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Map categories to numerical labels\ncategories = df['Category'].unique()\ncategory_to_idx = {category: idx for idx, category in enumerate(categories)}\ndf['Label'] = df['Category'].map(category_to_idx)\n\n# Split the dataset into train, validation, and test sets\ntrain_val_df, test_df = train_test_split(df, test_size=0.2, stratify=df['Label'])\ntrain_df, val_df = train_test_split(train_val_df, test_size=0.2, stratify=train_val_df['Label'])\n\n# Custom Dataset class for loading images and labels\nclass FruitVegDataset(Dataset):\n    def __init__(self, dataframe, transform=None):\n        self.dataframe = dataframe.reset_index(drop=True)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        img_path = self.dataframe.iloc[idx]['Image Directory']\n        image = Image.open(img_path).convert('RGB')\n        label = self.dataframe.iloc[idx]['Label']\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n# Define transformations for the training and validation sets\ntransform_train = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntransform_val_test = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Create datasets and dataloaders\ntrain_dataset = FruitVegDataset(train_df, transform=transform_train)\nval_dataset = FruitVegDataset(val_df, transform=transform_val_test)\ntest_dataset = FruitVegDataset(test_df, transform=transform_val_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)\ntest_loader = DataLoader(test_dataset, batch_size=16)\n\n# Load pre-trained EfficientNet B7 model and modify the final layer for our dataset\nmodel = models.efficientnet_b0(pretrained=True)\nmodel.classifier[1] = nn.Linear(model.classifier[1].in_features, len(categories))\nmodel.to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# Function to train and evaluate the model\ndef train_and_evaluate():\n    num_epochs = 1\n    best_accuracy = 0\n\n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        running_loss = 0.0\n        \n        for images, labels in tqdm(train_loader, desc=f'Training Epoch {epoch+1}/{num_epochs}'):\n            images, labels = images.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item() * images.size(0)\n            #print(f'Running loss {running_loss}')\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n\n        # Validation phase\n        model.eval()\n        corrects = 0\n        \n        with torch.no_grad():\n            for images, labels in tqdm(val_loader, desc=f'Validating'):\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                _, preds = torch.max(outputs, 1)\n                corrects += torch.sum(preds == labels).item()\n\n        epoch_accuracy = corrects / len(val_loader.dataset)\n\n        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n\n        # Save the best model based on validation accuracy\n        if epoch_accuracy > best_accuracy:\n            best_accuracy = epoch_accuracy\n            best_model_wts = model.state_dict()\n\n    # Load best model weights and evaluate on test set\n    model.load_state_dict(best_model_wts)\n    \n    # Test phase\n    model.eval()\n    test_corrects = 0\n    \n    with torch.no_grad():\n        for images, labels in tqdm(test_loader, desc=f'Testing'):\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n            test_corrects += torch.sum(preds == labels).item()\n\n    test_accuracy = test_corrects / len(test_loader.dataset)\n    print(f'Test Accuracy: {test_accuracy:.4f}')\n\n    # Save the trained model\n    torch.save(model.state_dict(), 'efficientnet_b0_fruit_veg-w.pth')\n    print(\"Model saved as 'efficientnet_b0_fruit_veg_2.pth\")\n\n# Train and evaluate the EfficientNet B7 model\ntrain_and_evaluate()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import transforms, models\nfrom PIL import Image\nimport pandas as pd\n\n# Ensure GPU usage if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\ncategories = df['Category'].unique()\ncategory_to_idx = {category: idx for idx, category in enumerate(categories)}\nidx_to_category = {idx: category for category, idx in category_to_idx.items()}\n\n# Load the saved model\nmodel = models.efficientnet_b0(pretrained=False)  # Load without pre-trained weights\nmodel.classifier[1] = nn.Linear(model.classifier[1].in_features, len(categories))\nmodel.load_state_dict(torch.load('/kaggle/working/efficientnet_b0_fruit_veg-w.pth', map_location=device))\nmodel.to(device)\nmodel.eval()\n\n# Define the transformation for the input image\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ndef predict_image_class(image_path):\n    image = Image.open(image_path).convert('RGB')\n    image = transform(image).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        outputs = model(image)\n        _, predicted_label = torch.max(outputs, 1)\n\n    predicted_label = predicted_label.item()\n    predicted_category = idx_to_category[predicted_label]\n\n    return predicted_label, predicted_category\n\n\n# Example usage:\nimage_path = '/kaggle/input/testing-of-me/RG.jpg'  # Replace with the path to your uploaded image\npredicted_label, predicted_category = predict_image_class(image_path)\n\nprint(\"Predicted label:\", predicted_label)\nprint(\"Predicted category:\", predicted_category)\n\n# Display the labelled classes and their actual names\nprint(\"\\nLabelled classes and their actual names:\")\nfor idx, category in idx_to_category.items():\n    print(f\"Label {idx}: {category}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Efficient Net For Sub Sub Category of Freshness","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# Ensure GPU usage if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Map categories to numerical labels\ncategories = df['Sub Sub Category'].unique()\ncategory_to_idx = {category: idx for idx, category in enumerate(categories)}\ndf['Label'] = df['Sub Sub Category'].map(category_to_idx)\n\n# Split the dataset into train, validation, and test sets\ntrain_val_df, test_df = train_test_split(df, test_size=0.2, stratify=df['Label'])\ntrain_df, val_df = train_test_split(train_val_df, test_size=0.2, stratify=train_val_df['Label'])\n\n# Custom Dataset class for loading images and labels\nclass FruitVegDataset(Dataset):\n    def __init__(self, dataframe, transform=None):\n        self.dataframe = dataframe.reset_index(drop=True)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        img_path = self.dataframe.iloc[idx]['Image Directory']\n        image = Image.open(img_path).convert('RGB')\n        label = self.dataframe.iloc[idx]['Label']\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n# Define transformations for the training and validation sets\ntransform_train = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntransform_val_test = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Create datasets and dataloaders\ntrain_dataset = FruitVegDataset(train_df, transform=transform_train)\nval_dataset = FruitVegDataset(val_df, transform=transform_val_test)\ntest_dataset = FruitVegDataset(test_df, transform=transform_val_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)\ntest_loader = DataLoader(test_dataset, batch_size=16)\n\n# Load pre-trained EfficientNet B7 model and modify the final layer for our dataset\nmodel = models.efficientnet_b0(pretrained=True)\nmodel.classifier[1] = nn.Linear(model.classifier[1].in_features, len(categories))\nmodel.to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# Function to train and evaluate the model\ndef train_and_evaluate():\n    num_epochs = 2\n    best_accuracy = 0\n\n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        running_loss = 0.0\n        \n        for images, labels in tqdm(train_loader, desc=f'Training Epoch {epoch+1}/{num_epochs}'):\n            images, labels = images.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item() * images.size(0)\n            #print(f'Running loss {running_loss}')\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n\n        # Validation phase\n        model.eval()\n        corrects = 0\n        \n        with torch.no_grad():\n            for images, labels in tqdm(val_loader, desc=f'Validating'):\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                _, preds = torch.max(outputs, 1)\n                corrects += torch.sum(preds == labels).item()\n\n        epoch_accuracy = corrects / len(val_loader.dataset)\n\n        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n\n        # Save the best model based on validation accuracy\n        if epoch_accuracy > best_accuracy:\n            best_accuracy = epoch_accuracy\n            best_model_wts = model.state_dict()\n\n    # Load best model weights and evaluate on test set\n    model.load_state_dict(best_model_wts)\n    \n    # Test phase\n    model.eval()\n    test_corrects = 0\n    \n    with torch.no_grad():\n        for images, labels in tqdm(test_loader, desc=f'Testing'):\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n            test_corrects += torch.sum(preds == labels).item()\n\n    test_accuracy = test_corrects / len(test_loader.dataset)\n    print(f'Test Accuracy: {test_accuracy:.4f}')\n\n    # Save the trained model\n    torch.save(model.state_dict(), 'efficientnet_b0_fruit_veg.pth')\n    print(\"Model saved as 'efficientnet_b0_fruit_veg.pth'\")\n\n# Train and evaluate the EfficientNet B7 model\ntrain_and_evaluate()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import transforms, models\nfrom PIL import Image\nimport pandas as pd\n\n# Ensure GPU usage if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\ncategories = df['Sub Sub Category'].unique()\ncategory_to_idx = {category: idx for idx, category in enumerate(categories)}\nidx_to_category = {idx: category for category, idx in category_to_idx.items()}\n\n# Load the saved model\nmodel = models.efficientnet_b0(pretrained=False)  # Load without pre-trained weights\nmodel.classifier[1] = nn.Linear(model.classifier[1].in_features, len(categories))\nmodel.load_state_dict(torch.load('/content/efficientnet_b0_fruit_veg.pth', map_location=device))\nmodel.to(device)\nmodel.eval()\n\n# Define the transformation for the input image\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ndef predict_image_class(image_path):\n    image = Image.open(image_path).convert('RGB')\n    image = transform(image).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        outputs = model(image)\n        _, predicted_label = torch.max(outputs, 1)\n\n    predicted_label = predicted_label.item()\n    predicted_category = idx_to_category[predicted_label]\n\n    return predicted_label, predicted_category\n\n\n# Example usage:\nimage_path = '/content/M.jpg'  # Replace with the path to your uploaded image\npredicted_label, predicted_category = predict_image_class(image_path)\n\nprint(\"Predicted label:\", predicted_label)\nprint(\"Predicted category:\", predicted_category)\n\n# Display the labelled classes and their actual names\nprint(\"\\nLabelled classes and their actual names:\")\nfor idx, category in idx_to_category.items():\n    print(f\"Label {idx}: {category}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}