{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1255086,"sourceType":"datasetVersion","datasetId":721614},{"sourceId":6358490,"sourceType":"datasetVersion","datasetId":2988671},{"sourceId":9574587,"sourceType":"datasetVersion","datasetId":5836668},{"sourceId":9665067,"sourceType":"datasetVersion","datasetId":5905421},{"sourceId":9665139,"sourceType":"datasetVersion","datasetId":5905475},{"sourceId":9665162,"sourceType":"datasetVersion","datasetId":5905496},{"sourceId":9674628,"sourceType":"datasetVersion","datasetId":5912640},{"sourceId":9674641,"sourceType":"datasetVersion","datasetId":5912651},{"sourceId":9675750,"sourceType":"datasetVersion","datasetId":5913500},{"sourceId":36559288,"sourceType":"kernelVersion"},{"sourceId":140519,"sourceType":"modelInstanceVersion","modelInstanceId":119019,"modelId":142270},{"sourceId":140601,"sourceType":"modelInstanceVersion","modelInstanceId":119089,"modelId":142340},{"sourceId":140804,"sourceType":"modelInstanceVersion","modelInstanceId":119260,"modelId":142508},{"sourceId":140924,"sourceType":"modelInstanceVersion","modelInstanceId":119356,"modelId":142604}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Define the main directories\npacked_dir = '/kaggle/input/freibergs-groceries/images'\nunpacked_dir = '/kaggle/input/woking-dataset/Detection'\n\n# Initialize an empty DataFrame\nimage_data = pd.DataFrame(columns=['image_position', 'label'])\n\n# Function to process images in a directory\ndef process_images(directory, label):\n    data = []\n    for root, dirs, files in os.walk(directory):\n        for filename in files:\n            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n                image_position = os.path.join(root, filename)\n                data.append({'image_position': image_position, 'label': label})\n    return pd.DataFrame(data)\n\n# Process packed images\npacked_data = process_images(packed_dir, 'Packed')\n\n# Process unpacked images\nunpacked_data = process_images(unpacked_dir, 'Unpacked')\n\n# Combine packed and unpacked data\nimage_data = pd.concat([packed_data, unpacked_data], ignore_index=True)\n\n# Display the first few rows of the DataFrame\nprint(image_data.head())\n\n# Display the total number of images\nprint(f\"\\nTotal number of images: {len(image_data)}\")\n\n# Display the number of packed and unpacked images\nprint(image_data['label'].value_counts())\n\n# Save the DataFrame to a CSV file\nimage_data.to_csv('image_data.csv', index=False)\nprint(\"\\nDataFrame saved to 'image_data.csv'\")","metadata":{"execution":{"iopub.status.busy":"2024-10-19T07:13:03.933501Z","iopub.execute_input":"2024-10-19T07:13:03.934165Z","iopub.status.idle":"2024-10-19T07:14:30.951710Z","shell.execute_reply.started":"2024-10-19T07:13:03.934124Z","shell.execute_reply":"2024-10-19T07:14:30.950729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-19T07:14:30.953525Z","iopub.execute_input":"2024-10-19T07:14:30.954010Z","iopub.status.idle":"2024-10-19T07:14:30.963846Z","shell.execute_reply.started":"2024-10-19T07:14:30.953964Z","shell.execute_reply":"2024-10-19T07:14:30.962956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Define the main directories\npacked_dir = '/kaggle/input/freibergs-groceries/images'\nunpacked_dir = '/kaggle/input/woking-dataset/Detection'\nnew_packed_dir = '/kaggle/input/grocery-store-dataset/GroceryStoreDataset/dataset'\n\n# Initialize an empty DataFrame\nimage_data = pd.DataFrame(columns=['image_position', 'label'])\n\n# Function to process images in a directory\ndef process_images(directory, label):\n    data = []\n    for root, dirs, files in os.walk(directory):\n        for filename in files:\n            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n                image_position = os.path.join(root, filename)\n                data.append({'image_position': image_position, 'label': label})\n    return pd.DataFrame(data)\n\n# Process original packed images\npacked_data = process_images(packed_dir, 'Packed')\n\n# Process unpacked images\nunpacked_data = process_images(unpacked_dir, 'Unpacked')\n\n# Process new packed images from GroceryStoreDataset\nnew_packed_data = pd.DataFrame()\nfor category in ['train', 'val', 'test']:\n    packages_dir = os.path.join(new_packed_dir, category, 'Packages')\n    if os.path.exists(packages_dir):\n        new_packed_data = pd.concat([new_packed_data, process_images(packages_dir, 'Packed')], ignore_index=True)\n\n# Combine all data\nimage_data = pd.concat([packed_data, unpacked_data, new_packed_data], ignore_index=True)\n\n# Display the first few rows of the DataFrame\nprint(image_data.head())\n\n# Display the total number of images\nprint(f\"\\nTotal number of images: {len(image_data)}\")\n\n# Display the number of packed and unpacked images\nprint(\"\\nCount of packed and unpacked images:\")\nprint(image_data['label'].value_counts())\n\n# Save the DataFrame to a CSV file\nimage_data.to_csv('image_data.csv', index=False)\nprint(\"\\nDataFrame saved to 'image_data.csv'\")","metadata":{"execution":{"iopub.status.busy":"2024-10-19T07:34:43.480166Z","iopub.execute_input":"2024-10-19T07:34:43.480938Z","iopub.status.idle":"2024-10-19T07:36:16.954932Z","shell.execute_reply.started":"2024-10-19T07:34:43.480896Z","shell.execute_reply":"2024-10-19T07:36:16.953978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-19T07:36:25.401776Z","iopub.execute_input":"2024-10-19T07:36:25.402154Z","iopub.status.idle":"2024-10-19T07:36:25.411664Z","shell.execute_reply.started":"2024-10-19T07:36:25.402117Z","shell.execute_reply":"2024-10-19T07:36:25.410648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\nCount of packed and unpacked images:\")\nprint(image_data['label'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-10-19T07:36:34.635827Z","iopub.execute_input":"2024-10-19T07:36:34.636492Z","iopub.status.idle":"2024-10-19T07:36:34.663297Z","shell.execute_reply.started":"2024-10-19T07:36:34.636454Z","shell.execute_reply":"2024-10-19T07:36:34.662324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.utils import resample\n\n# Assuming you already have your image_data DataFrame\n# If not, load it from your CSV file\n# image_data = pd.read_csv('image_data.csv')\n\n# Get the counts of packed and unpacked images\npacked_count = image_data[image_data['label'] == 'Packed'].shape[0]\nunpacked_count = image_data[image_data['label'] == 'Unpacked'].shape[0]\n\nprint(\"Original distribution:\")\nprint(f\"Packed: {packed_count}\")\nprint(f\"Unpacked: {unpacked_count}\")\n\n# Determine the minimum count for balancing the dataset\nmin_count = min(packed_count, unpacked_count)\n\n# Stratify the DataFrame by randomly sampling\npacked_sampled = image_data[image_data['label'] == 'Packed'].sample(n=min_count, random_state=42)\nunpacked_sampled = image_data[image_data['label'] == 'Unpacked'].sample(n=min_count, random_state=42)\n\n# Create a balanced DataFrame\nbalanced_image_data = pd.concat([packed_sampled, unpacked_sampled], ignore_index=True)\n\n# Shuffle the DataFrame\nbalanced_image_data = balanced_image_data.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# Display the new distribution\nprint(\"\\nBalanced distribution:\")\nprint(balanced_image_data['label'].value_counts())\n\n# Save the balanced DataFrame to a new CSV file\nbalanced_image_data.to_csv('balanced_image_data.csv', index=False)\nprint(\"\\nBalanced DataFrame saved to 'balanced_image_data.csv'\")","metadata":{"execution":{"iopub.status.busy":"2024-10-19T07:38:23.356268Z","iopub.execute_input":"2024-10-19T07:38:23.356669Z","iopub.status.idle":"2024-10-19T07:38:24.012344Z","shell.execute_reply.started":"2024-10-19T07:38:23.356622Z","shell.execute_reply":"2024-10-19T07:38:24.011448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# Ensure GPU usage if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load the balanced dataset\ndf = pd.read_csv('/kaggle/working/balanced_dataset.csv')\n\n# Map categories to numerical labels\ncategory_to_idx = {'Packed': 0, 'Unpacked': 1}\ndf['Label'] = df['label'].map(category_to_idx)\n\n# Split the dataset into train, validation, and test sets\ntrain_val_df, test_df = train_test_split(df, test_size=0.2, stratify=df['Label'])\ntrain_df, val_df = train_test_split(train_val_df, test_size=0.2, stratify=train_val_df['Label'])\n\n# Custom Dataset class for loading images and labels\nclass PackedUnpackedDataset(Dataset):\n    def __init__(self, dataframe, transform=None):\n        self.dataframe = dataframe.reset_index(drop=True)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        img_path = self.dataframe.iloc[idx]['image_position']\n        image = Image.open(img_path).convert('RGB')\n        label = self.dataframe.iloc[idx]['Label']\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n# Define transformations for the training and validation sets\ntransform_train = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntransform_val_test = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Create datasets and dataloaders\ntrain_dataset = PackedUnpackedDataset(train_df, transform=transform_train)\nval_dataset = PackedUnpackedDataset(val_df, transform=transform_val_test)\ntest_dataset = PackedUnpackedDataset(test_df, transform=transform_val_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)\ntest_loader = DataLoader(test_dataset, batch_size=16)\n\n# Load pre-trained EfficientNet B0 model and modify the final layer for our dataset\nmodel = models.efficientnet_b0(pretrained=True)\nmodel.classifier[1] = nn.Linear(model.classifier[1].in_features, 2)  # 2 classes: Packed and Unpacked\nmodel.to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# Function to train and evaluate the model\ndef train_and_evaluate():\n    num_epochs = 3\n    best_accuracy = 0\n\n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        running_loss = 0.0\n        \n        for images, labels in tqdm(train_loader, desc=f'Training Epoch {epoch+1}/{num_epochs}'):\n            images, labels = images.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item() * images.size(0)\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n\n        # Validation phase\n        model.eval()\n        corrects = 0\n        \n        with torch.no_grad():\n            for images, labels in tqdm(val_loader, desc=f'Validating'):\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                _, preds = torch.max(outputs, 1)\n                corrects += torch.sum(preds == labels).item()\n\n        epoch_accuracy = corrects / len(val_loader.dataset)\n\n        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n\n        # Save the best model based on validation accuracy\n        if epoch_accuracy > best_accuracy:\n            best_accuracy = epoch_accuracy\n            best_model_wts = model.state_dict()\n\n    # Load best model weights and evaluate on test set\n    model.load_state_dict(best_model_wts)\n    \n    # Test phase\n    model.eval()\n    test_corrects = 0\n    \n    with torch.no_grad():\n        for images, labels in tqdm(test_loader, desc=f'Testing'):\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n            test_corrects += torch.sum(preds == labels).item()\n\n    test_accuracy = test_corrects / len(test_loader.dataset)\n    print(f'Test Accuracy: {test_accuracy:.4f}')\n\n    # Save the trained model\n    torch.save(model.state_dict(), 'efficientnet_b0_packed_unpacked.pth')\n    print(\"Model saved as 'efficientnet_b0_packed_unpacked.pth'\")\n\n# Train and evaluate the EfficientNet B0 model\ntrain_and_evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-10-19T08:47:11.280892Z","iopub.execute_input":"2024-10-19T08:47:11.281267Z","iopub.status.idle":"2024-10-19T08:47:12.626847Z","shell.execute_reply.started":"2024-10-19T08:47:11.281233Z","shell.execute_reply":"2024-10-19T08:47:12.625265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import transforms, models\nfrom PIL import Image\n\n# Ensure GPU usage if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Define categories\ncategories = ['Packed', 'Unpacked']\ncategory_to_idx = {category: idx for idx, category in enumerate(categories)}\nidx_to_category = {idx: category for category, idx in category_to_idx.items()}\n\n# Load the saved model\nmodel = models.efficientnet_b0(pretrained=False)  # Load without pre-trained weights\nmodel.classifier[1] = nn.Linear(model.classifier[1].in_features, len(categories))\nmodel.load_state_dict(torch.load('/kaggle/working/efficientnet_b0_packed_unpacked.pth', map_location=device))\nmodel.to(device)\nmodel.eval()\n\n# Define the transformation for the input image\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ndef predict_image_class(image_path):\n    image = Image.open(image_path).convert('RGB')\n    image = transform(image).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        outputs = model(image)\n        _, predicted_label = torch.max(outputs, 1)\n\n    predicted_label = predicted_label.item()\n    predicted_category = idx_to_category[predicted_label]\n\n    return predicted_label, predicted_category\n\n# Example usage:\nimage_path = '/kaggle/input/woking-dataset/Detection/Fruits/Beetroot/Beetroot 1/100_100.jpg'  # Replace with the path to your uploaded image\npredicted_label, predicted_category = predict_image_class(image_path)\n\nprint(\"Predicted label:\", predicted_label)\nprint(\"Predicted category:\", predicted_category)\n\n# Display the labelled classes and their actual names\nprint(\"\\nLabelled classes and their actual names:\")\nfor idx, category in idx_to_category.items():\n    print(f\"Label {idx}: {category}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-19T08:17:41.734595Z","iopub.execute_input":"2024-10-19T08:17:41.734984Z","iopub.status.idle":"2024-10-19T08:17:41.983508Z","shell.execute_reply.started":"2024-10-19T08:17:41.734947Z","shell.execute_reply":"2024-10-19T08:17:41.982491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport csv\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef categorize_image(directory):\n    if 'S_' in directory or 'Bad' in directory or 'Rotten' in directory or 'Old' in directory or 'Dried' in directory or 'Damaged' in directory or 'Formalin-mixed' in directory:\n        return 'Rotten'\n    else:\n        return 'Fresh'\n\ndef create_csv(root_dir, output_file):\n    with open(output_file, 'w', newline='') as csvfile:\n        fieldnames = ['Image Directory', 'Main Directory', 'Category', 'Sub Category', 'Sub Sub Category']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n\n        for main_dir in ['Fruits', 'Vegetables']:\n            main_path = os.path.join(root_dir, 'Detection', main_dir)\n            \n            for category in os.listdir(main_path):\n                category_path = os.path.join(main_path, category)\n                \n                if os.path.isdir(category_path):\n                    for sub_category in os.listdir(category_path):\n                        sub_category_path = os.path.join(category_path, sub_category)\n                        \n                        if os.path.isdir(sub_category_path):\n                            sub_sub_category = categorize_image(sub_category)\n                            \n                            for image in os.listdir(sub_category_path):\n                                if image.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n                                    image_path = os.path.join(sub_category_path, image)\n                                    writer.writerow({\n                                        'Image Directory': image_path,\n                                        'Main Directory': main_dir,\n                                        'Category': category,\n                                        'Sub Category': sub_category,\n                                        'Sub Sub Category': sub_sub_category\n                                    })\n\ndef create_balanced_df(unpacked_csv, packed_csv1, packed_csv2, output_csv):\n    # Read the CSVs\n    unpacked_df = pd.read_csv(unpacked_csv)\n    packed_df1 = pd.read_csv(packed_csv1)\n    packed_df2 = pd.read_csv(packed_csv2)\n\n    # Combine packed datasets\n    packed_df = pd.concat([packed_df1, packed_df2], ignore_index=True)\n    packed_df['label'] = 'Packed'\n\n    # Add 'label' column to unpacked_df\n    unpacked_df['label'] = 'Unpacked'\n\n    # Get the number of packed images\n    n_packed = len(packed_df)\n\n    # Stratified sampling of unpacked images\n    unpacked_stratified = unpacked_df.groupby(['Main Directory', 'Category', 'Sub Category', 'Sub Sub Category'])\n    unpacked_sampled = unpacked_stratified.apply(lambda x: x.sample(n=int(np.rint(n_packed * len(x) / len(unpacked_df))), replace=True))\n    unpacked_sampled = unpacked_sampled.reset_index(drop=True)\n\n    # Combine packed and unpacked datasets\n    combined_df = pd.concat([packed_df, unpacked_sampled], ignore_index=True)\n\n    # Shuffle the combined dataset\n    combined_df = combined_df.sample(frac=1).reset_index(drop=True)\n\n    # Save the combined dataset\n    combined_df.to_csv(output_csv, index=False)\n\n    return combined_df\n\n# Usage\nroot_directory = '/kaggle/input/woking-dataset'\nunpacked_csv = 'fruit_vegetable_dataset.csv'\ncreate_csv(root_directory, unpacked_csv)\n\npacked_csv1 = '/path/to/packed_dataset1.csv'  # Replace with actual path\npacked_csv2 = '/path/to/packed_dataset2.csv'  # Replace with actual path\noutput_csv = 'balanced_dataset.csv'\n\nbalanced_df = create_balanced_df(unpacked_csv, packed_csv1, packed_csv2, output_csv)\n\nprint(f\"Balanced dataset created with {len(balanced_df)} images\")\nprint(balanced_df['label'].value_counts())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef collect_freiberg_images(root_dir):\n    images = []\n    for category in tqdm(os.listdir(root_dir), desc=\"Processing Freiberg dataset\"):\n        category_path = os.path.join(root_dir, category)\n        if os.path.isdir(category_path):\n            for root, _, files in os.walk(category_path):\n                for file in files:\n                    if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n                        image_path = os.path.join(root, file)\n                        images.append({\n                            'Image Directory': image_path,\n                            'Label': 'Packed'\n                        })\n    return pd.DataFrame(images)\n\ndef collect_woking_images(root_dir):\n    images = []\n    for main_dir in tqdm(['Fruits', 'Vegetables'], desc=\"Processing Woking dataset\"):\n        main_path = os.path.join(root_dir, main_dir)\n        for category in os.listdir(main_path):\n            category_path = os.path.join(main_path, category)\n            if os.path.isdir(category_path):\n                for sub_category in os.listdir(category_path):\n                    sub_category_path = os.path.join(category_path, sub_category)\n                    if os.path.isdir(sub_category_path):\n                        for root, _, files in os.walk(sub_category_path):\n                            for file in files:\n                                if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n                                    image_path = os.path.join(root, file)\n                                    images.append({\n                                        'Image Directory': image_path,\n                                        'Main Directory': main_dir,\n                                        'Category': category,\n                                        'Sub Category': sub_category,\n                                        'Label': 'Unpacked'\n                                    })\n    return pd.DataFrame(images)\n\ndef collect_grocery_images(root_dir):\n    images = []\n    for sub_dir in tqdm(['iconic-images-and-descriptions', 'test', 'train', 'val'], desc=\"Processing GroceryStore dataset\"):\n        sub_dir_path = os.path.join(root_dir, sub_dir)\n        if os.path.exists(sub_dir_path):\n            packages_path = os.path.join(sub_dir_path, 'Packages')\n            if os.path.exists(packages_path):\n                for root, _, files in os.walk(packages_path):\n                    for file in files:\n                        if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n                            image_path = os.path.join(root, file)\n                            images.append({\n                                'Image Directory': image_path,\n                                'Label': 'Packed'\n                            })\n    return pd.DataFrame(images)\n\n# Paths to datasets\nfreiberg_dir = '/kaggle/input/freibergs-groceries/images'\nwoking_dir = '/kaggle/input/woking-dataset/Detection'\ngrocery_dir = '/kaggle/input/grocery-store-dataset/GroceryStoreDataset/dataset'\n\n# Collect images from all datasets\nfreiberg_df = collect_freiberg_images(freiberg_dir)\nwoking_df = collect_woking_images(woking_dir)\ngrocery_df = collect_grocery_images(grocery_dir)\n\n# Combine packed datasets\npacked_df = pd.concat([freiberg_df, grocery_df], ignore_index=True)\n\n# Get the number of packed images\nn_packed = len(packed_df)\n\n# Stratified sampling of unpacked images\nunpacked_stratified = woking_df.groupby(['Main Directory', 'Category', 'Sub Category'])\nunpacked_sampled = unpacked_stratified.apply(lambda x: x.sample(n=int(np.rint(n_packed * len(x) / len(woking_df))), replace=True))\nunpacked_sampled = unpacked_sampled.reset_index(drop=True)\n\n# Ensure exact same number of packed and unpacked images\nif len(unpacked_sampled) > n_packed:\n    unpacked_sampled = unpacked_sampled.sample(n=n_packed, random_state=42)\nelif len(unpacked_sampled) < n_packed:\n    packed_df = packed_df.sample(n=len(unpacked_sampled), random_state=42)\n\n# Combine packed and unpacked datasets\ncombined_df = pd.concat([packed_df, unpacked_sampled], ignore_index=True)\n\n# Keep only 'Image Directory' and 'Label' columns\ncombined_df = combined_df[['Image Directory', 'Label']]\n\n# Shuffle the combined dataset\ncombined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# Display counts\npacked_count = combined_df[combined_df['Label'] == 'Packed'].shape[0]\nunpacked_count = combined_df[combined_df['Label'] == 'Unpacked'].shape[0]\n\nprint(f\"Number of packed images: {packed_count}\")\nprint(f\"Number of unpacked images: {unpacked_count}\")\nprint(f\"Total number of images: {len(combined_df)}\")\n\n# Save the combined DataFrame to a CSV file\ncombined_df.to_csv('balanced_dataset.csv', index=False)\nprint(\"Balanced dataset saved to 'balanced_dataset.csv'\")","metadata":{"execution":{"iopub.status.busy":"2024-10-19T08:44:48.381992Z","iopub.execute_input":"2024-10-19T08:44:48.382387Z","iopub.status.idle":"2024-10-19T08:45:19.698675Z","shell.execute_reply.started":"2024-10-19T08:44:48.382351Z","shell.execute_reply":"2024-10-19T08:45:19.697787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-19T08:47:50.863166Z","iopub.execute_input":"2024-10-19T08:47:50.863553Z","iopub.status.idle":"2024-10-19T08:47:50.873713Z","shell.execute_reply.started":"2024-10-19T08:47:50.863516Z","shell.execute_reply":"2024-10-19T08:47:50.872696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODEL FOR EFFICIENT NET FOR PACKED UNPACKED","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# Ensure GPU usage if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load the balanced dataset\ndf = pd.read_csv('/kaggle/working/balanced_dataset.csv')\n\n# Map categories to numerical labels\ncategory_to_idx = {'Packed': 0, 'Unpacked': 1}\ndf['Label'] = df['Label'].map(category_to_idx)\n\n# Split the dataset into train, validation, and test sets\ntrain_val_df, test_df = train_test_split(df, test_size=0.2, stratify=df['Label'])\ntrain_df, val_df = train_test_split(train_val_df, test_size=0.2, stratify=train_val_df['Label'])\n\n# Custom Dataset class for loading images and labels\nclass PackedUnpackedDataset(Dataset):\n    def __init__(self, dataframe, transform=None):\n        self.dataframe = dataframe.reset_index(drop=True)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        img_path = self.dataframe.iloc[idx]['Image Directory']\n        image = Image.open(img_path).convert('RGB')\n        label = self.dataframe.iloc[idx]['Label']\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n# Define transformations for the training and validation sets\ntransform_train = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntransform_val_test = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Create datasets and dataloaders\ntrain_dataset = PackedUnpackedDataset(train_df, transform=transform_train)\nval_dataset = PackedUnpackedDataset(val_df, transform=transform_val_test)\ntest_dataset = PackedUnpackedDataset(test_df, transform=transform_val_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)\ntest_loader = DataLoader(test_dataset, batch_size=16)\n\n# Load pre-trained EfficientNet B0 model and modify the final layer for our dataset\nmodel = models.efficientnet_b0(pretrained=True)\nmodel.classifier[1] = nn.Linear(model.classifier[1].in_features, 2)  # 2 classes: Packed and Unpacked\nmodel.to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# Function to train and evaluate the model\ndef train_and_evaluate():\n    num_epochs = 1\n    best_accuracy = 0\n\n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        running_loss = 0.0\n        \n        for images, labels in tqdm(train_loader, desc=f'Training Epoch {epoch+1}/{num_epochs}'):\n            images, labels = images.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item() * images.size(0)\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n\n        # Validation phase\n        model.eval()\n        corrects = 0\n        \n        with torch.no_grad():\n            for images, labels in tqdm(val_loader, desc=f'Validating'):\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                _, preds = torch.max(outputs, 1)\n                corrects += torch.sum(preds == labels).item()\n\n        epoch_accuracy = corrects / len(val_loader.dataset)\n\n        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n\n        # Save the best model based on validation accuracy\n        if epoch_accuracy > best_accuracy:\n            best_accuracy = epoch_accuracy\n            best_model_wts = model.state_dict()\n\n    # Load best model weights and evaluate on test set\n    model.load_state_dict(best_model_wts)\n    \n    # Test phase\n    model.eval()\n    test_corrects = 0\n    \n    with torch.no_grad():\n        for images, labels in tqdm(test_loader, desc=f'Testing'):\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n            test_corrects += torch.sum(preds == labels).item()\n\n    test_accuracy = test_corrects / len(test_loader.dataset)\n    print(f'Test Accuracy: {test_accuracy:.4f}')\n\n    # Save the trained model\n    torch.save(model.state_dict(), 'efficientnet_b0_packed_unpacked1.pth')\n    print(\"Model saved as 'efficientnet_b0_packed_unpacked1.pth'\")\n\n# Train and evaluate the EfficientNet B0 model\ntrain_and_evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:11:14.310945Z","iopub.execute_input":"2024-10-19T09:11:14.311689Z","iopub.status.idle":"2024-10-19T09:13:50.608343Z","shell.execute_reply.started":"2024-10-19T09:11:14.311650Z","shell.execute_reply":"2024-10-19T09:13:50.607320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"/kaggle/working/efficientnet_b0_packed_unpacked.pth","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import transforms, models\nfrom PIL import Image\n\n# Ensure GPU usage if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Define categories\ncategories = ['Packed', 'Unpacked']\ncategory_to_idx = {category: idx for idx, category in enumerate(categories)}\nidx_to_category = {idx: category for category, idx in category_to_idx.items()}\n\n# Load the saved model\nmodel = models.efficientnet_b0(pretrained=False)  # Load without pre-trained weights\nmodel.classifier[1] = nn.Linear(model.classifier[1].in_features, len(categories))\nmodel.load_state_dict(torch.load('/kaggle/working/efficientnet_b0_packed_unpacked.pth', map_location=device))\nmodel.to(device)\nmodel.eval()\n\n# Define the transformation for the input image\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ndef predict_image_class(image_path):\n    image = Image.open(image_path).convert('RGB')\n    image = transform(image).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        outputs = model(image)\n        _, predicted_label = torch.max(outputs, 1)\n\n    predicted_label = predicted_label.item()\n    predicted_category = idx_to_category[predicted_label]\n\n    return predicted_label, predicted_category\n\n# Example usage:\nimage_path = '/kaggle/input/grocery-store-dataset/GroceryStoreDataset/dataset/test/Vegetables/Ginger/Ginger_001.jpg'  # Replace with the path to your uploaded image\npredicted_label, predicted_category = predict_image_class(image_path)\n\nprint(\"Predicted label:\", predicted_label)\nprint(\"Predicted category:\", predicted_category)\n\n# Display the labelled classes and their actual names\nprint(\"\\nLabelled classes and their actual names:\")\nfor idx, category in idx_to_category.items():\n    print(f\"Label {idx}: {category}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-19T09:01:03.919655Z","iopub.execute_input":"2024-10-19T09:01:03.920536Z","iopub.status.idle":"2024-10-19T09:01:04.169023Z","shell.execute_reply.started":"2024-10-19T09:01:03.920494Z","shell.execute_reply":"2024-10-19T09:01:04.168024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# STANDARD SEGMENTATION","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-10-20T06:09:36.607572Z","iopub.execute_input":"2024-10-20T06:09:36.608161Z","iopub.status.idle":"2024-10-20T06:09:50.214993Z","shell.execute_reply.started":"2024-10-20T06:09:36.608119Z","shell.execute_reply":"2024-10-20T06:09:50.214044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ultralytics import SAM, YOLO\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nimport pandas as pd\nfrom PIL import Image\nimport os\nfrom sklearn.metrics import jaccard_score\n\ndef iou(mask1, mask2):\n    return jaccard_score(mask1.flatten(), mask2.flatten())\n\ndef place_on_white_background(image):\n    white_bg = np.ones_like(image) * 255\n    mask = np.any(image != [0, 0, 0], axis=-1)\n    white_bg[mask] = image[mask]\n    return white_bg\n\ndef process_sam_results(results, model_name, yolo_model, iou_threshold=0.8, confidence_threshold=0.25):\n    original_image = results[0].orig_img\n    masks = results[0].masks.data.cpu().numpy()\n    boxes = results[0].boxes.data.cpu().numpy()\n    \n    unique_objects = []\n    for i, (mask, box) in enumerate(zip(masks, boxes)):\n        is_unique = True\n        for existing_obj in unique_objects:\n            if iou(mask, existing_obj['mask']) > iou_threshold:\n                is_unique = False\n                break\n        \n        if is_unique:\n            object_mask = mask.astype(bool)\n            object_image = np.zeros_like(original_image)\n            object_image[object_mask] = original_image[object_mask]\n            \n            x1, y1, x2, y2 = map(int, box[:4])\n            cropped_object = object_image[y1:y2, x1:x2]\n            \n            # Place on white background\n            object_on_white = place_on_white_background(cropped_object)\n            \n            # Perform YOLO detection on the cropped object\n            yolo_results = yolo_model(object_on_white)\n            yolo_boxes = yolo_results[0].boxes\n            \n            # Check if any detection has confidence above the threshold\n            is_product = any(conf > confidence_threshold for conf in yolo_boxes.conf)\n            \n            object_filename = f\"{model_name}object{len(unique_objects)}.png\"\n            cv2.imwrite(object_filename, cv2.cvtColor(object_on_white, cv2.COLOR_RGB2BGR))\n            \n            unique_objects.append({\n                'model': model_name,\n                'object_id': len(unique_objects),\n                'image': Image.fromarray(object_on_white),\n                'mask': mask,\n                'x1': x1,\n                'y1': y1,\n                'x2': x2,\n                'y2': y2,\n                'filename': object_filename,\n                'is_product': is_product\n            })\n    \n    return unique_objects\n\ndef visualize_objects(objects, cols=5):\n    n = len(objects)\n    rows = (n + cols - 1) // cols\n    fig, axs = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n    axs = axs.flatten()\n    \n    for i, obj in enumerate(objects):\n        axs[i].imshow(obj['image'])\n        axs[i].axis('off')\n        title = f\"Object {obj['object_id']}\\n{'Product' if obj['is_product'] else 'Not Product'}\"\n        axs[i].set_title(title, color='green' if obj['is_product'] else 'red')\n    \n    for i in range(n, len(axs)):\n        axs[i].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Load models\nsam_model_b = SAM(\"sam2_b.pt\")\nyolo_model = YOLO(\"/kaggle/input/nothing-new/pytorch/default/1/yolov8x-oiv7 (2).pt\")  # or the path to your YOLO model\n\n# Load image\nimg_path = \"/kaggle/input/new-tester/Imashe.jpg\"\nimg = cv2.imread(img_path)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n# Process with SAM2_b model\nresults_b = sam_model_b(img)\n\n# Display segmentation results\nplt.figure(figsize=(10, 10))\nplt.imshow(results_b[0].plot())\nplt.title(\"Segmentation results - SAM2_b\")\nplt.axis('off')\nplt.show()\n\n# Process and visualize objects\nobjects_b = process_sam_results(results_b, \"sam2_b\", yolo_model)\nvisualize_objects(objects_b)\n\n# Create DataFrame\ndf_b = pd.DataFrame([{k: v for k, v in obj.items() if k != 'mask' and k != 'image'} for obj in objects_b])\n\n# Display DataFrame\nprint(df_b)\n\n# Save DataFrame to CSV\ndf_b.to_csv('segmented_objects_sam2_b.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-20T06:44:39.556571Z","iopub.execute_input":"2024-10-20T06:44:39.556977Z","iopub.status.idle":"2024-10-20T06:44:51.249254Z","shell.execute_reply.started":"2024-10-20T06:44:39.556939Z","shell.execute_reply":"2024-10-20T06:44:51.248335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# COMPLETE","metadata":{}},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms, models\nfrom PIL import Image\nimport pandas as pd\n\n# Ensure GPU usage if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\ncategories =['Fresh', 'Rotten']\ncategory_to_idx = {category: idx for idx, category in enumerate(categories)}\nidx_to_category = {idx: category for category, idx in category_to_idx.items()}\n\n# Load the saved model\nmodel = models.efficientnet_b0(pretrained=False)  # Load without pre-trained weights\nmodel.classifier[1] = nn.Linear(model.classifier[1].in_features, len(categories))\nmodel.load_state_dict(torch.load('/kaggle/input/fresh-model/keras/default/1/efficientnet_b0_fruit_veg (2).pth', map_location=device))\nmodel.to(device)\nmodel.eval()\n\n# Define the transformation for the input image\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ndef predict_image_class(image_path):\n    image = Image.open(image_path).convert('RGB')\n    image = transform(image).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        outputs = model(image)\n        print(outputs)\n        print(torch.max(outputs,1))\n        _, predicted_label = torch.max(outputs, 1)\n\n    predicted_label = predicted_label.item()\n    predicted_category = idx_to_category[predicted_label]\n\n    return predicted_label, predicted_category\n\n\n# Example usage:\nimage_path = '/kaggle/input/woking-dataset/Detection/Fruits/Pear/Rotten/Image1.png'  # Replace with the path to your uploaded image\npredicted_label, predicted_category = predict_image_class(image_path)\n\nprint(\"Predicted label:\", predicted_label)\nprint(\"Predicted category:\", predicted_category)\n\n# Display the labelled classes and their actual names\nprint(\"\\nLabelled classes and their actual names:\")\nfor idx, category in idx_to_category.items():\n    print(f\"Label {idx}: {category}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-19T16:05:48.591320Z","iopub.execute_input":"2024-10-19T16:05:48.591678Z","iopub.status.idle":"2024-10-19T16:05:49.205592Z","shell.execute_reply.started":"2024-10-19T16:05:48.591644Z","shell.execute_reply":"2024-10-19T16:05:49.204542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('ello')","metadata":{"execution":{"iopub.status.busy":"2024-10-19T17:52:50.683348Z","iopub.execute_input":"2024-10-19T17:52:50.683691Z","iopub.status.idle":"2024-10-19T17:52:50.694017Z","shell.execute_reply.started":"2024-10-19T17:52:50.683655Z","shell.execute_reply":"2024-10-19T17:52:50.693218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install ultralytics","metadata":{"execution":{"iopub.status.busy":"2024-10-19T23:19:23.871191Z","iopub.execute_input":"2024-10-19T23:19:23.871810Z","iopub.status.idle":"2024-10-19T23:19:38.270079Z","shell.execute_reply.started":"2024-10-19T23:19:23.871770Z","shell.execute_reply":"2024-10-19T23:19:38.269107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom ultralytics import SAM\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nimport pandas as pd\nfrom PIL import Image\nimport os\nfrom sklearn.metrics import jaccard_score\nfrom transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n\ndef iou(mask1, mask2):\n    return jaccard_score(mask1.flatten(), mask2.flatten())\n\ndef place_on_white_background(image):\n    white_bg = np.ones_like(image) * 255\n    mask = np.any(image != [0, 0, 0], axis=-1)\n    white_bg[mask] = image[mask]\n    return white_bg\n\ndef resize_image(image, min_size=224):\n    width, height = image.size\n    if width < min_size or height < min_size:\n        scale = min_size / min(width, height)\n        new_width = int(width * scale)\n        new_height = int(height * scale)\n        return image.resize((new_width, new_height), Image.LANCZOS)\n    return image\n\ndef is_product(image, model, processor):\n    image = resize_image(image)\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\"},\n                {\"type\": \"text\", \"text\": \"Is this image showing a packed product or fruits or vegetables ? Answer only yes or no.\"}\n            ]\n        }\n    ]\n    text_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n    inputs = processor(text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\")\n    inputs = inputs.to(model.device)\n    output_ids = model.generate(**inputs, max_new_tokens=10)\n    output_text = processor.batch_decode(output_ids, skip_special_tokens=True)[0].strip().lower()\n    return \"yes\" in output_text\n\ndef process_sam_results(results, model_name, qwen_model, qwen_processor, iou_threshold=0.8, min_object_size=28):\n    original_image = results[0].orig_img\n    masks = results[0].masks.data.cpu().numpy()\n    boxes = results[0].boxes.data.cpu().numpy()\n    \n    unique_objects = []\n    for i, (mask, box) in enumerate(zip(masks, boxes)):\n        is_unique = True\n        for existing_obj in unique_objects:\n            if iou(mask, existing_obj['mask']) > iou_threshold:\n                is_unique = False\n                break\n        \n        if is_unique:\n            object_mask = mask.astype(bool)\n            object_image = np.zeros_like(original_image)\n            object_image[object_mask] = original_image[object_mask]\n            \n            x1, y1, x2, y2 = map(int, box[:4])\n            cropped_object = object_image[y1:y2, x1:x2]\n            \n            # Skip extremely small objects\n            if cropped_object.shape[0] < min_object_size or cropped_object.shape[1] < min_object_size:\n                continue\n            \n            object_on_white = place_on_white_background(cropped_object)\n            pil_image = Image.fromarray(object_on_white)\n            \n            is_product_result = is_product(pil_image, qwen_model, qwen_processor)\n            \n            if is_product_result:\n                object_filename = f\"{model_name}object{len(unique_objects)}.png\"\n                cv2.imwrite(object_filename, cv2.cvtColor(object_on_white, cv2.COLOR_RGB2BGR))\n                \n                unique_objects.append({\n                    'model': model_name,\n                    'object_id': len(unique_objects),\n                    'image': pil_image,\n                    'mask': mask,\n                    'x1': x1,\n                    'y1': y1,\n                    'x2': x2,\n                    'y2': y2,\n                    'filename': object_filename,\n                    'is_product': is_product_result\n                })\n    \n    return unique_objects\n\ndef visualize_objects(objects, cols=5):\n    n = len(objects)\n    rows = (n + cols - 1) // cols\n    fig, axs = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n    axs = axs.flatten()\n    \n    for i, obj in enumerate(objects):\n        axs[i].imshow(obj['image'])\n        axs[i].axis('off')\n        axs[i].set_title(f\"Object {obj['object_id']}\")\n    \n    for i in range(n, len(axs)):\n        axs[i].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Load models\nsam_model = SAM(\"sam2_b.pt\")\nqwen_model = Qwen2VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2-VL-2B-Instruct\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\",\n)\nqwen_processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n\n# Load image\nimg_path = \"/kaggle/input/newest-tester-data/download (3).jpg\"\nimg = cv2.imread(img_path)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n# Display original image\nplt.figure(figsize=(10, 10))\nplt.imshow(img)\nplt.title(\"Original Image\")\nplt.axis('off')\nplt.show()\n\n# Process with SAM model\nresults = sam_model(img)\n\n# Display segmentation results\nplt.figure(figsize=(10, 10))\nplt.imshow(results[0].plot())\nplt.title(\"Segmentation results - SAM\")\nplt.axis('off')\nplt.show()\n\n# Process and visualize objects\nobjects = process_sam_results(results, \"sam\", qwen_model, qwen_processor)\nvisualize_objects(objects)\n\n# Create DataFrame\ndf = pd.DataFrame([{k: v for k, v in obj.items() if k != 'mask' and k != 'image'} for obj in objects])\n\n# Display DataFrame\nprint(df)\n\n# Save DataFrame to CSV\ndf.to_csv('segmented_products.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-20T06:11:11.866968Z","iopub.execute_input":"2024-10-20T06:11:11.867323Z","iopub.status.idle":"2024-10-20T06:12:37.546041Z","shell.execute_reply.started":"2024-10-20T06:11:11.867289Z","shell.execute_reply":"2024-10-20T06:12:37.545158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Complete working","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import transforms, models\nfrom ultralytics import SAM\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nimport pandas as pd\nfrom PIL import Image\nimport os\nfrom sklearn.metrics import jaccard_score\nfrom transformers import Qwen2VLForConditionalGeneration, AutoProcessor\nimport warnings\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Ensure GPU usage if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\ndef iou(mask1, mask2):\n    return jaccard_score(mask1.flatten(), mask2.flatten())\n\ndef place_on_white_background(image):\n    white_bg = np.ones_like(image) * 255\n    mask = np.any(image != [0, 0, 0], axis=-1)\n    white_bg[mask] = image[mask]\n    return white_bg\n\ndef process_sam_results(results, model_name, iou_threshold=0.99):\n    original_image = results[0].orig_img\n    masks = results[0].masks.data.cpu().numpy()\n    boxes = results[0].boxes.data.cpu().numpy()\n    \n    unique_objects = []\n    for i, (mask, box) in enumerate(zip(masks, boxes)):\n        is_unique = True\n        for existing_obj in unique_objects:\n            if iou(mask, existing_obj['mask']) > iou_threshold:\n                is_unique = False\n                break\n        \n        if is_unique:\n            object_mask = mask.astype(bool)\n            object_image = np.zeros_like(original_image)\n            object_image[object_mask] = original_image[object_mask]\n            \n            x1, y1, x2, y2 = map(int, box[:4])\n            cropped_object = object_image[y1:y2, x1:x2]\n            \n            object_on_white = place_on_white_background(cropped_object)\n            \n            object_filename = f\"{model_name}object{len(unique_objects)}.png\"\n            cv2.imwrite(object_filename, cv2.cvtColor(object_on_white, cv2.COLOR_RGB2BGR))\n            \n            unique_objects.append({\n                'model': model_name,\n                'object_id': len(unique_objects),\n                'image': Image.fromarray(object_on_white),\n                'mask': mask,\n                'x1': x1,\n                'y1': y1,\n                'x2': x2,\n                'y2': y2,\n                'filename': object_filename\n            })\n    \n    return unique_objects\n\ndef visualize_objects(objects, cols=5):\n    n = len(objects)\n    rows = (n + cols - 1) // cols\n    fig, axs = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n    axs = axs.flatten()\n    \n    for i, obj in enumerate(objects):\n        axs[i].imshow(obj['image'])\n        axs[i].axis('off')\n        axs[i].set_title(f\"Object {obj['object_id']}\")\n    \n    for i in range(n, len(axs)):\n        axs[i].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Load and initialize models\nsam_model_b = SAM(\"sam2_b.pt\")\n\npacked_unpacked_model = models.efficientnet_b0(pretrained=False)\npacked_unpacked_model.classifier[1] = nn.Linear(packed_unpacked_model.classifier[1].in_features, 2)\npacked_unpacked_model.load_state_dict(torch.load('/kaggle/input/models-loaded/keras/default/1/efficientnet_b0_packed_unpacked (2).pth', map_location=device))\npacked_unpacked_model.to(device)\npacked_unpacked_model.eval()\n\nfruit_veg_model = models.efficientnet_b0(pretrained=False)\nfruit_veg_model.classifier[1] = nn.Linear(fruit_veg_model.classifier[1].in_features, 54)\nfruit_veg_model.load_state_dict(torch.load('/kaggle/input/models-loaded/keras/default/1/efficientnet_b0_fruit_veg_1 (1).pth', map_location=device))\nfruit_veg_model.to(device)\nfruit_veg_model.eval()\n\nfresh_rotten_model = models.efficientnet_b0(pretrained=False)\nfresh_rotten_model.classifier[1] = nn.Linear(fresh_rotten_model.classifier[1].in_features, 2)\nfresh_rotten_model.load_state_dict(torch.load('/kaggle/input/models-loaded/keras/default/1/efficientnet_b0_fruit_veg (2).pth', map_location=device))\nfresh_rotten_model.to(device)\nfresh_rotten_model.eval()\n\nqwen_model = Qwen2VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2-VL-2B-Instruct\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\",\n)\nqwen_processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n\n# Define transformations and categories\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\npacked_unpacked_categories = ['Packed', 'Unpacked']\npacked_unpacked_idx_to_category = {idx: category for idx, category in enumerate(packed_unpacked_categories)}\n\nfruit_veg_categories = [\n    'Orange', 'Tamarillo', 'Lime', 'Pomegranate', 'Plum', 'Pineapple', 'Apple', 'Dates', 'Papaya', 'Guava',\n    'Beetroot', 'Pear', 'Strawberry', 'Blueberry', 'Lulo', 'Avacado', 'Lemon', 'Kaki', 'Peach', 'Grape',\n    'Banana', 'Cherry', 'Watermelon', 'Mango', 'Grapefruit', 'Broccoli', 'Capsicum', 'Radish', 'Tomato', 'Turnip',\n    'Ginger', 'Zucchini', 'Brinjal', 'Pumpkin', 'Bell Pepper', 'Carrot', 'New Mexico Green Chile', 'Eggplant',\n    'Baby Corn', 'Zucchini dark', 'Sweet corn', 'Cabbage', 'Bitter_Gourd', 'Cauliflower', 'Chile Pepper',\n    'Sweet Potato', 'Bean', 'Cucumber', 'Bottle Gourd', 'Garlic', 'Peas', 'Onion', 'Potato', 'Spinach'\n]\nfruit_veg_idx_to_category = {idx: category for idx, category in enumerate(fruit_veg_categories)}\n\nfresh_rotten_categories = ['Fresh', 'Rotten']\nfresh_rotten_idx_to_category = {idx: category for idx, category in enumerate(fresh_rotten_categories)}\n\npacked_categories = [\n    'Staples', 'Snacks & Beverages', 'Packaged Food', 'Personal & Baby Care',\n    'Household Care', 'Dairy & Eggs', 'Home & Kitchen'\n]\n\ndef predict_image_class(model, image_path, idx_to_category):\n    try:\n        image = Image.open(image_path).convert('RGB')\n        image = transform(image).unsqueeze(0).to(device)\n\n        with torch.no_grad():\n            outputs = model(image)\n            _, predicted_label = torch.max(outputs, 1)\n\n        predicted_label = predicted_label.item()\n        predicted_category = idx_to_category[predicted_label]\n\n        return predicted_label, predicted_category\n    except Exception as e:\n        print(f\"Error predicting class for {image_path}: {str(e)}\")\n        return None, None\n\ndef get_product_info(image_path, question):\n    try:\n        image = Image.open(image_path)\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"image\"},\n                    {\"type\": \"text\", \"text\": question}\n                ]\n            }\n        ]\n        text_prompt = qwen_processor.apply_chat_template(messages, add_generation_prompt=True)\n        inputs = qwen_processor(text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\")\n        inputs = inputs.to(\"cuda\")\n        output_ids = qwen_model.generate(**inputs, max_new_tokens=1024)\n        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n        output_text = qwen_processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n        return output_text[0]\n    except Exception as e:\n        print(f\"Error getting product info for {image_path}: {str(e)}\")\n        return \"Error: Unable to process image\"\n\ndef process_image(image_path):\n    try:\n        # Load image\n        img = cv2.imread(image_path)\n        if img is None:\n            raise ValueError(f\"Unable to load image: {image_path}\")\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        # Perform initial segmentation\n        results_b = sam_model_b(img)\n        objects_b = process_sam_results(results_b, \"sam2_b\")\n\n        # Create DataFrame for segmented objects\n        df_objects = pd.DataFrame([{k: v for k, v in obj.items() if k != 'mask' and k != 'image'} for obj in objects_b])\n        df_objects.to_csv('segmented_objects.csv', index=False)\n\n        # Initialize DataFrames for packed and unpacked items\n        df_packed = pd.DataFrame(columns=['name', 'expiry_date', 'description', 'frequency', 'category'])\n        df_unpacked = pd.DataFrame(columns=['name', 'frequency', 'condition', 'indepth_condition', 'weight'])\n\n        # Process each segmented object\n        for _, obj in df_objects.iterrows():\n            object_image_path = obj['filename']\n            \n            # Determine if the object is packed or unpacked\n            _, packed_unpacked_category = predict_image_class(packed_unpacked_model, object_image_path, packed_unpacked_idx_to_category)\n            \n            if packed_unpacked_category == 'Unpacked':\n                # Classify fruit/vegetable\n                _, fruit_veg_category = predict_image_class(fruit_veg_model, object_image_path, fruit_veg_idx_to_category)\n                \n                # Determine if fresh or rotten\n                _, fresh_rotten_category = predict_image_class(fresh_rotten_model, object_image_path, fresh_rotten_idx_to_category)\n                \n                # Add or update entry in df_unpacked\n                if fruit_veg_category in df_unpacked['name'].values:\n                    df_unpacked.loc[df_unpacked['name'] == fruit_veg_category, 'frequency'] += 1\n                else:\n                    new_row = pd.DataFrame({\n                        'name': [fruit_veg_category],\n                        'frequency': [1],\n                        'condition': [fresh_rotten_category],\n                        'indepth_condition': ['To be determined'],  # Placeholder for now\n                        'weight': ['To be determined']  # Placeholder for now\n                    })\n                    df_unpacked = pd.concat([df_unpacked, new_row], ignore_index=True)\n            \n            elif packed_unpacked_category == 'Packed':\n                # Get product information using Qwen model\n                product_name = get_product_info(object_image_path, \"What is the name of the product? NOTE: JUST PROVIDE NAME AS THE ANSWER\")\n                expiry_date = get_product_info(object_image_path, \"What is the expiry date of the product? If not visible, say 'Not visible'\")\n                description = get_product_info(object_image_path, \"Provide a brief description of the product\")\n                \n                # Determine category (placeholder logic, replace with actual categorization)\n                category = packed_categories[0]  # Default to first category\n                \n                # Add or update entry in df_packed\n                if product_name in df_packed['name'].values:\n                    df_packed.loc[df_packed['name'] == product_name, 'frequency'] += 1\n                else:\n                    new_row = pd.DataFrame({\n                        'name': [product_name],\n                        'expiry_date': [expiry_date],\n                        'description': [description],\n                        'frequency': [1],\n                        'category': [category]\n                    })\n                    df_packed = pd.concat([df_packed, new_row], ignore_index=True)\n\n        # Save results\n        df_packed.to_csv('packed_items.csv', index=False)\n        df_unpacked.to_csv('unpacked_items.csv', index=False)\n\n        return df_packed, df_unpacked, df_objects\n    except Exception as e:\n        print(f\"Error processing image: {str(e)}\")\n        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n\n# Example usage\nimage_path = \"/kaggle/input/newest-tester-data/lay-s-american-style-cream-and-onion-potato-chips-32-g-quick-pantry.jpg\"\ndf_packed, df_unpacked, df_objects = process_image(image_path)\n\nprint(\"Packed items:\")\nprint(df_packed)\nprint(\"\\nUnpacked items:\")\nprint(df_unpacked)\nprint(\"\\nAll segmented objects:\")\nprint(df_objects)","metadata":{"execution":{"iopub.status.busy":"2024-10-20T00:30:33.612335Z","iopub.execute_input":"2024-10-20T00:30:33.612718Z","iopub.status.idle":"2024-10-20T00:31:06.317354Z","shell.execute_reply.started":"2024-10-20T00:30:33.612679Z","shell.execute_reply":"2024-10-20T00:31:06.316327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Trying for segmentation","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ultralytics import YOLO, SAM\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\nfrom skimage.filters import sobel\nfrom skimage.measure import label, regionprops\n\ndef detect_and_segment(image_path, yolo_model, sam_model, confidence_threshold=0.25, iou_threshold=0.5, \n                       min_area=1000, blur_threshold=50, edge_threshold=0.1):\n    # Step 1: Object Detection with YOLOv8\n    results = yolo_model(image_path)[0]\n    detections = results.boxes.data.cpu().numpy()\n\n    # Read the image\n    image = cv2.imread(image_path)\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    segmented_objects = []\n\n    for detection in detections:\n        x1, y1, x2, y2, conf, class_id = detection\n        if conf < confidence_threshold:\n            continue\n\n        # Step 2: Apply SAM2 for segmentation\n        sam_result = sam_model(image_rgb, bboxes=[[x1, y1, x2, y2]])[0]\n        mask = sam_result.masks.data[0].cpu().numpy()\n\n        # Check mask quality\n        if not is_mask_valid(mask, min_area, blur_threshold, edge_threshold):\n            continue\n\n        # Check for overlap with existing objects\n        if any(calculate_iou(mask, existing_obj['mask']) > iou_threshold for existing_obj in segmented_objects):\n            continue\n\n        # Extract the object using the mask\n        object_image = np.zeros_like(image_rgb)\n        object_image[mask] = image_rgb[mask]\n\n        # Crop the object\n        y_indices, x_indices = np.where(mask)\n        x_min, x_max = np.min(x_indices), np.max(x_indices)\n        y_min, y_max = np.min(y_indices), np.max(y_indices)\n        cropped_object = object_image[y_min:y_max, x_min:x_max]\n\n        # Add segmented object to the list\n        segmented_objects.append({\n            'class_id': int(class_id),\n            'confidence': conf,\n            'image': cropped_object,\n            'mask': mask\n        })\n\n    return segmented_objects\n\ndef is_mask_valid(mask, min_area, blur_threshold, edge_threshold):\n    # Check area\n    if np.sum(mask) < min_area:\n        return False\n\n    # Check for blurriness\n    gray_mask = (mask * 255).astype(np.uint8)\n    laplacian_var = cv2.Laplacian(gray_mask, cv2.CV_64F).var()\n    if laplacian_var < blur_threshold:\n        return False\n\n    # Check for edge strength\n    edges = sobel(mask)\n    if np.mean(edges) < edge_threshold:\n        return False\n\n    # Check for compactness and shape regularity\n    labeled_mask = label(mask)\n    regions = regionprops(labeled_mask)\n    if regions:\n        main_region = max(regions, key=lambda r: r.area)\n        if main_region.eccentricity > 0.95 or main_region.solidity < 0.5:\n            return False\n\n    return True\n\ndef calculate_iou(mask1, mask2):\n    intersection = np.logical_and(mask1, mask2)\n    union = np.logical_or(mask1, mask2)\n    return np.sum(intersection) / np.sum(union)\n\ndef save_and_display_objects(segmented_objects, yolo_model, output_dir='segmented_objects'):\n    os.makedirs(output_dir, exist_ok=True)\n    \n    plt.figure(figsize=(20, 20))\n    for i, obj in enumerate(segmented_objects):\n        # Save the object image\n        filename = f\"{output_dir}/object_{i}_{yolo_model.names[obj['class_id']]}.png\"\n        cv2.imwrite(filename, cv2.cvtColor(obj['image'], cv2.COLOR_RGB2BGR))\n        \n        # Display the object image\n        plt.subplot(5, 5, i+1)\n        plt.imshow(obj['image'])\n        plt.title(f\"{yolo_model.names[obj['class_id']]}\\nConf: {obj['confidence']:.2f}\")\n        plt.axis('off')\n        \n        if i == 24:  # Limit to 25 images for display\n            break\n    \n    plt.tight_layout()\n    plt.show()\n\n# Load models\nyolo_model = YOLO('yolo11x.pt')\nsam_model = SAM('sam2_b.pt')\n\n# Process an image\nimage_path = '/kaggle/input/newest-tester-data/images.jpg'\nsegmented_objects = detect_and_segment(image_path, yolo_model, sam_model)\n\n# Save and display the segmented objects\nsave_and_display_objects(segmented_objects, yolo_model)\n\n# Print information about segmented objects\nfor i, obj in enumerate(segmented_objects):\n    print(f\"Object {i+1}:\")\n    print(f\"  Class: {yolo_model.names[obj['class_id']]}\")\n    print(f\"  Confidence: {obj['confidence']:.2f}\")\n    print(f\"  Image Shape: {obj['image'].shape}\")\n    print()","metadata":{"execution":{"iopub.status.busy":"2024-10-20T06:04:07.289938Z","iopub.execute_input":"2024-10-20T06:04:07.290331Z","iopub.status.idle":"2024-10-20T06:04:09.486896Z","shell.execute_reply.started":"2024-10-20T06:04:07.290291Z","shell.execute_reply":"2024-10-20T06:04:09.486052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# NEW","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-10-20T06:47:32.534331Z","iopub.execute_input":"2024-10-20T06:47:32.534763Z","iopub.status.idle":"2024-10-20T06:47:52.087829Z","shell.execute_reply.started":"2024-10-20T06:47:32.534720Z","shell.execute_reply":"2024-10-20T06:47:52.086419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch,gc\n\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-10-20T06:01:13.611100Z","iopub.execute_input":"2024-10-20T06:01:13.611777Z","iopub.status.idle":"2024-10-20T06:01:14.488604Z","shell.execute_reply.started":"2024-10-20T06:01:13.611734Z","shell.execute_reply":"2024-10-20T06:01:14.487590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom ultralytics import SAM\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nimport pandas as pd\nfrom PIL import Image\nimport os\nfrom sklearn.metrics import jaccard_score\nfrom transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n\n# Set the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\ndef iou(mask1, mask2):\n    return jaccard_score(mask1.flatten(), mask2.flatten())\n\ndef place_on_white_background(image):\n    white_bg = np.ones_like(image) * 255\n    mask = np.any(image != [0, 0, 0], axis=-1)\n    white_bg[mask] = image[mask]\n    return white_bg\n\ndef resize_image(image, min_size=224):\n    width, height = image.size\n    if width < min_size or height < min_size:\n        scale = min_size / min(width, height)\n        new_width = int(width * scale)\n        new_height = int(height * scale)\n        return image.resize((new_width, new_height), Image.LANCZOS)\n    return image\n\ndef is_product(image, model, processor):\n    image = resize_image(image)\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\"},\n                {\"type\": \"text\", \"text\": \"Is this image showing a product , if its a label or qr code or a tag type or just plain white or anything that isnt an object its a no? Answer only yes or no.\"}\n            ]\n        }\n    ]\n    text_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n    inputs = processor(text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\")\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    output_ids = model.generate(**inputs, max_new_tokens=10)\n    output_text = processor.batch_decode(output_ids, skip_special_tokens=True)[0].strip().lower()\n    return \"yes\" in output_text\n\ndef process_sam_results(results, model_name, qwen_model, qwen_processor, iou_threshold=0.8, min_object_size=28):\n    original_image = results[0].orig_img\n    masks = results[0].masks.data.cpu().numpy()\n    boxes = results[0].boxes.data.cpu().numpy()\n    \n    unique_objects = []\n    for i, (mask, box) in enumerate(zip(masks, boxes)):\n        is_unique = True\n        for existing_obj in unique_objects:\n            if iou(mask, existing_obj['mask']) > iou_threshold:\n                is_unique = False\n                break\n        \n        if is_unique:\n            object_mask = mask.astype(bool)\n            object_image = np.zeros_like(original_image)\n            object_image[object_mask] = original_image[object_mask]\n            \n            x1, y1, x2, y2 = map(int, box[:4])\n            cropped_object = object_image[y1:y2, x1:x2]\n            \n            # Skip extremely small objects\n            if cropped_object.shape[0] < min_object_size or cropped_object.shape[1] < min_object_size:\n                continue\n            \n            object_on_white = place_on_white_background(cropped_object)\n            pil_image = Image.fromarray(object_on_white)\n            \n            is_product_result = is_product(pil_image, qwen_model, qwen_processor)\n            \n            if is_product_result:\n                object_filename = f\"{model_name}object{len(unique_objects)}.png\"\n                cv2.imwrite(object_filename, cv2.cvtColor(object_on_white, cv2.COLOR_RGB2BGR))\n                \n                unique_objects.append({\n                    'model': model_name,\n                    'object_id': len(unique_objects),\n                    'image': pil_image,\n                    'mask': mask,\n                    'x1': x1,\n                    'y1': y1,\n                    'x2': x2,\n                    'y2': y2,\n                    'filename': object_filename,\n                    'is_product': is_product_result\n                })\n    \n    return unique_objects\n\ndef visualize_objects(objects, cols=5):\n    n = len(objects)\n    rows = (n + cols - 1) // cols\n    fig, axs = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n    axs = axs.flatten()\n    \n    for i, obj in enumerate(objects):\n        axs[i].imshow(obj['image'])\n        axs[i].axis('off')\n        axs[i].set_title(f\"Object {obj['object_id']}\")\n    \n    for i in range(n, len(axs)):\n        axs[i].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Load models\nsam_model = SAM(\"sam2_b.pt\").to(device)\nqwen_model = Qwen2VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2-VL-2B-Instruct\",\n    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n    device_map=\"auto\",\n)\nqwen_processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n\n# Load image\nimg_path = \"/kaggle/input/newest-tester-data/download (4).jpg\"\nimg = cv2.imread(img_path)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n# Display original image\nplt.figure(figsize=(10, 10))\nplt.imshow(img)\nplt.title(\"Original Image\")\nplt.axis('off')\nplt.show()\n\n# Process with SAM model\nresults = sam_model(img)\n\n# Display segmentation results\nplt.figure(figsize=(10, 10))\nplt.imshow(results[0].plot())\nplt.title(\"Segmentation results - SAM\")\nplt.axis('off')\nplt.show()\n\n# Process and visualize objects\nobjects = process_sam_results(results, \"sam\", qwen_model, qwen_processor)\nvisualize_objects(objects)\n\n# Create DataFrame\ndf = pd.DataFrame([{k: v for k, v in obj.items() if k != 'mask' and k != 'image'} for obj in objects])\n\n# Display DataFrame\nprint(df)\n\n# Save DataFrame to CSV\ndf.to_csv('segmented_products.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1","metadata":{}},{"cell_type":"code","source":"from ultralytics import SAM\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nimport pandas as pd\nfrom PIL import Image\nimport os\nfrom sklearn.metrics import jaccard_score\n\ndef iou(mask1, mask2):\n    return jaccard_score(mask1.flatten(), mask2.flatten())\n\ndef place_on_white_background(image):\n    # Create a white background\n    white_bg = np.ones_like(image) * 255\n    \n    # Create a mask for non-black pixels\n    mask = np.any(image != [0, 0, 0], axis=-1)\n    \n    # Place the image on the white background\n    white_bg[mask] = image[mask]\n    \n    return white_bg\n\ndef process_sam_results(results, model_name, iou_threshold=0.8):\n    original_image = results[0].orig_img\n    masks = results[0].masks.data.cpu().numpy()\n    boxes = results[0].boxes.data.cpu().numpy()\n    \n    unique_objects = []\n    for i, (mask, box) in enumerate(zip(masks, boxes)):\n        is_unique = True\n        for existing_obj in unique_objects:\n            if iou(mask, existing_obj['mask']) > iou_threshold:\n                is_unique = False\n                break\n        \n        if is_unique:\n            object_mask = mask.astype(bool)\n            object_image = np.zeros_like(original_image)\n            object_image[object_mask] = original_image[object_mask]\n            \n            x1, y1, x2, y2 = map(int, box[:4])\n            cropped_object = object_image[y1:y2, x1:x2]\n            \n            # Place on white background\n            object_on_white = place_on_white_background(cropped_object)\n            \n            object_filename = f\"{model_name}object{len(unique_objects)}.png\"\n            cv2.imwrite(object_filename, cv2.cvtColor(object_on_white, cv2.COLOR_RGB2BGR))\n            \n            unique_objects.append({\n                'model': model_name,\n                'object_id': len(unique_objects),\n                'image': Image.fromarray(object_on_white),\n                'mask': mask,\n                'x1': x1,\n                'y1': y1,\n                'x2': x2,\n                'y2': y2,\n                'filename': object_filename\n            })\n    \n    return unique_objects\n\ndef visualize_objects(objects, cols=5):\n    n = len(objects)\n    rows = (n + cols - 1) // cols\n    fig, axs = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n    axs = axs.flatten()\n    \n    for i, obj in enumerate(objects):\n        axs[i].imshow(obj['image'])\n        axs[i].axis('off')\n        axs[i].set_title(f\"Object {obj['object_id']}\")\n    \n    for i in range(n, len(axs)):\n        axs[i].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Load image\nimg_path = '/kaggle/input/new-tester/top-view-grapefruit-with-oranges-pink-background_141793-51393.jpg'\nimg = cv2.imread(img_path)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n# Process with SAM2_b model\nsam_model_b = SAM(\"sam2_b.pt\")\nresults_b = sam_model_b(img)\n\n# Display segmentation results\nplt.figure(figsize=(10, 10))\nplt.imshow(results_b[0].plot())\nplt.title(\"Segmentation results - SAM2_b\")\nplt.axis('off')\nplt.show()\n\n# Process and visualize objects\nobjects_b = process_sam_results(results_b, \"sam2_b\")\nvisualize_objects(objects_b)\n\n# Create DataFrame\ndf_b = pd.DataFrame([{k: v for k, v in obj.items() if k != 'mask' and k != 'image'} for obj in objects_b])\n\n# Display DataFrame\nprint(df_b)\n\n# Save DataFrame to CSV\ndf_b.to_csv('segmented_objects_sam2_b.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-20T06:53:10.244980Z","iopub.execute_input":"2024-10-20T06:53:10.245847Z","iopub.status.idle":"2024-10-20T06:53:28.417277Z","shell.execute_reply.started":"2024-10-20T06:53:10.245804Z","shell.execute_reply":"2024-10-20T06:53:28.416343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom ultralytics import SAM\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nimport pandas as pd\nfrom PIL import Image\nfrom sklearn.metrics import jaccard_score\nfrom torchvision.models import resnet50\nfrom torchvision.transforms import functional as F\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\ndef iou(mask1, mask2):\n    return jaccard_score(mask1.flatten(), mask2.flatten())\n\ndef place_on_white_background(image):\n    white_bg = np.ones_like(image) * 255\n    mask = np.any(image != [0, 0, 0], axis=-1)\n    white_bg[mask] = image[mask]\n    return white_bg\n\ndef process_sam_results(results, sam_model_name, iou_threshold=0.5, min_object_size=28):\n    original_image = results[0].orig_img\n    masks = results[0].masks.data.cpu().numpy()\n    \n    unique_objects = []\n    for i, mask in enumerate(masks):\n        is_unique = True\n        for existing_obj in unique_objects:\n            if iou(mask, existing_obj['mask']) > iou_threshold:\n                is_unique = False\n                break\n        \n        if is_unique:\n            object_mask = mask.astype(bool)\n            object_image = np.zeros_like(original_image)\n            object_image[object_mask] = original_image[object_mask]\n            \n            # Get bounding box\n            y, x = np.where(object_mask)\n            y1, y2, x1, x2 = y.min(), y.max(), x.min(), x.max()\n            \n            cropped_object = object_image[y1:y2, x1:x2]\n            \n            # Skip extremely small objects\n            if cropped_object.shape[0] < min_object_size or cropped_object.shape[1] < min_object_size:\n                continue\n            \n            object_on_white = place_on_white_background(cropped_object)\n            \n            object_filename = f\"{sam_model_name}object{len(unique_objects)}.png\"\n            cv2.imwrite(object_filename, cv2.cvtColor(object_on_white, cv2.COLOR_RGB2BGR))\n            \n            unique_objects.append({\n                'model': sam_model_name,\n                'object_id': len(unique_objects),\n                'image': Image.fromarray(object_on_white),\n                'mask': mask,\n                'bbox': [x1, y1, x2, y2],\n                'filename': object_filename\n            })\n    \n    return unique_objects\n\ndef visualize_objects(objects, cols=5):\n    n = len(objects)\n    rows = (n + cols - 1) // cols\n    fig, axs = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n    axs = axs.flatten()\n    \n    for i, obj in enumerate(objects):\n        axs[i].imshow(obj['image'])\n        axs[i].axis('off')\n        axs[i].set_title(f\"Object {obj['object_id']}\")\n    \n    for i in range(n, len(axs)):\n        axs[i].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\ndef is_object(image, model):\n    # Preprocess the image\n    image = F.to_tensor(image)\n    image = F.normalize(image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    image = image.unsqueeze(0).to(device)\n    \n    # Get model prediction\n    with torch.no_grad():\n        output = model(image)\n    \n    # Check if the top predicted class is not background (assuming background is class 0)\n    _, predicted = output.max(1)\n    return predicted.item() != 0\n\n# Load SAM model\nsam_model = SAM(\"sam2_b.pt\").to(device)\n\n# Load pre-trained ResNet model for object classification\nresnet_model = resnet50(pretrained=True).to(device)\nresnet_model.eval()\n\n# Load image\nimg_path = \"/kaggle/input/new-tester/flat-sticker-pattern-fruits-white-background_1156689-7863.jpg\"\nimg = cv2.imread(img_path)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n# Display original image\nplt.figure(figsize=(10, 10))\nplt.imshow(img)\nplt.title(\"Original Image\")\nplt.axis('off')\nplt.show()\n\n# Process with SAM model\nresults = sam_model(img)\n\n# Display segmentation results\nplt.figure(figsize=(10, 10))\nplt.imshow(results[0].plot())\nplt.title(\"Segmentation results - SAM\")\nplt.axis('off')\nplt.show()\n\n# Process SAM results\nobjects = process_sam_results(results, \"sam\")\n\nprint(f\"Number of objects detected by SAM: {len(objects)}\")\n\n# Visualize objects from SAM\nvisualize_objects(objects)\n\n# Filter objects using ResNet\nfiltered_objects = [obj for obj in objects if is_object(obj['image'], resnet_model)]\n\nprint(f\"Number of objects after filtering: {len(filtered_objects)}\")\n\n# Visualize filtered objects\nvisualize_objects(filtered_objects)\n\n# Create DataFrame\ndf = pd.DataFrame([{k: v for k, v in obj.items() if k not in ['mask', 'image']} for obj in filtered_objects])\n\n# Display DataFrame\nprint(df)\n\n# Save DataFrame to CSV\ndf.to_csv('segmented_objects.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-20T06:57:03.517572Z","iopub.execute_input":"2024-10-20T06:57:03.518479Z","iopub.status.idle":"2024-10-20T06:58:04.865968Z","shell.execute_reply.started":"2024-10-20T06:57:03.518438Z","shell.execute_reply":"2024-10-20T06:58:04.865019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import DetrForObjectDetection, DetrImageProcessor\nfrom ultralytics import SAM\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\ndef detect_and_segment(image_path, detr_model, detr_processor, sam_model, confidence_threshold=0.7):\n    # Load the image\n    image = Image.open(image_path)\n    image_np = np.array(image)\n    \n    # Step 1: Object Detection with DETR\n    inputs = detr_processor(images=image, return_tensors=\"pt\")\n    outputs = detr_model(**inputs)\n    \n    # Post-process DETR outputs\n    target_sizes = torch.tensor([image.size[::-1]])\n    results = detr_processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=confidence_threshold)[0]\n    \n    segmented_objects = []\n    image_with_boxes = image_np.copy()\n    \n    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n        box = [round(i, 2) for i in box.tolist()]\n        if score >= confidence_threshold:\n            # Step 2: Apply SAM for segmentation\n            sam_result = sam_model(image_np, bboxes=[box])[0]\n            mask = sam_result.masks.data[0].cpu().numpy()\n            \n            # Add segmented object to the list\n            segmented_objects.append({\n                'class_id': label.item(),\n                'class_name': detr_model.config.id2label[label.item()],\n                'confidence': score.item(),\n                'bbox': box,\n                'mask': mask\n            })\n            \n            # Visualize the segmentation\n            colored_mask = np.zeros_like(image_np)\n            colored_mask[mask] = [0, 255, 0]  # Green color for the mask\n            image_with_boxes = cv2.addWeighted(image_with_boxes, 1, colored_mask, 0.5, 0)\n            \n            # Draw bounding box\n            cv2.rectangle(image_with_boxes, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (255, 0, 0), 2)\n            \n            # Add label\n            label_text = f\"{detr_model.config.id2label[label.item()]}: {score:.2f}\"\n            cv2.putText(image_with_boxes, label_text, (int(box[0]), int(box[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n    \n    return image_with_boxes, segmented_objects\n\n# Load models\ndetr_model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\ndetr_processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\nsam_model = SAM('sam2_b.pt')  # or 'sam2_l.pt' for a larger model\n\n# Process an image\nimage_path = '/path/to/your/image.jpg'\nresult_image, segmented_objects = detect_and_segment(image_path, detr_model, detr_processor, sam_model)\n\n# Display the original image\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.imshow(Image.open(image_path))\nplt.title(\"Original Image\")\nplt.axis('off')\n\n# Display the result image\nplt.subplot(1, 2, 2)\nplt.imshow(result_image)\nplt.title(\"Detected and Segmented Objects\")\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n\n# Save the result image\noutput_path = 'result_image.jpg'\ncv2.imwrite(output_path, cv2.cvtColor(result_image, cv2.COLOR_RGB2BGR))\n\n# Print information about segmented objects\nfor i, obj in enumerate(segmented_objects):\n    print(f\"Object {i+1}:\")\n    print(f\"  Class: {obj['class_name']}\")\n    print(f\"  Confidence: {obj['confidence']:.2f}\")\n    print(f\"  Bounding Box: {obj['bbox']}\")\n    print(f\"  Mask Shape: {obj['mask'].shape}\")\n    print()\n\n# Visualize individual segmented objects\nnum_objects = len(segmented_objects)\ncols = 3\nrows = (num_objects + cols - 1) // cols\nplt.figure(figsize=(15, 5 * rows))\n\nfor i, obj in enumerate(segmented_objects):\n    plt.subplot(rows, cols, i + 1)\n    \n    # Create a masked image\n    masked_image = np.zeros_like(result_image)\n    masked_image[obj['mask']] = result_image[obj['mask']]\n    \n    plt.imshow(masked_image)\n    plt.title(f\"{obj['class_name']} ({obj['confidence']:.2f})\")\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install ultralytics","metadata":{"execution":{"iopub.status.busy":"2024-10-20T12:11:30.769565Z","iopub.execute_input":"2024-10-20T12:11:30.770437Z","iopub.status.idle":"2024-10-20T12:11:48.316594Z","shell.execute_reply.started":"2024-10-20T12:11:30.770386Z","shell.execute_reply":"2024-10-20T12:11:48.315342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom ultralytics import SAM\nimport matplotlib.pyplot as plt\nfrom skimage.measure import label, regionprops\nfrom skimage.filters import threshold_otsu\n\ndef preprocess_image(image):\n    # Apply Gaussian blur to reduce noise\n    blurred = cv2.GaussianBlur(image, (5, 5), 0)\n    \n    # Enhance contrast using CLAHE\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n    enhanced = clahe.apply(cv2.cvtColor(blurred, cv2.COLOR_BGR2GRAY))\n    \n    return enhanced\n\ndef is_valid_object(mask, min_area=100, min_solidity=0.5):\n    # Label connected components in the mask\n    labeled = label(mask)\n    regions = regionprops(labeled)\n    \n    if not regions:\n        return False\n    \n    # Check area and solidity of the largest region\n    largest_region = max(regions, key=lambda r: r.area)\n    return largest_region.area >= min_area and largest_region.solidity >= min_solidity\n\ndef extract_object(image, mask):\n    # Create a white background\n    white_bg = np.ones_like(image) * 255\n    \n    # Place the object on the white background\n    object_image = np.where(mask[:,:,None], image, white_bg)\n    \n    # Crop the object\n    y, x = np.where(mask)\n    return object_image[np.min(y):np.max(y), np.min(x):np.max(x)]\n\ndef segment_and_filter_objects(image_path, sam_model):\n    # Load and preprocess the image\n    image = cv2.imread(image_path)\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    preprocessed = preprocess_image(image)\n    \n    # Generate SAM masks\n    sam_result = sam_model(image_rgb)[0]\n    masks = sam_result.masks.data.cpu().numpy()\n    \n    # Filter and extract objects\n    valid_objects = []\n    for mask in masks:\n        if is_valid_object(mask):\n            object_image = extract_object(image_rgb, mask)\n            valid_objects.append(object_image)\n    \n    return valid_objects\n\ndef display_objects(objects, title):\n    cols = 5\n    rows = (len(objects) + cols - 1) // cols\n    plt.figure(figsize=(15, 3 * rows))\n    plt.suptitle(title, fontsize=16)\n    \n    for i, obj in enumerate(objects):\n        plt.subplot(rows, cols, i + 1)\n        plt.imshow(obj)\n        plt.axis('off')\n        plt.title(f\"Object {i+1}\")\n    \n    plt.tight_layout()\n    plt.show()\n\n# Load SAM model\nsam_model = SAM('sam2_b.pt')\n\n# Process image\nimage_path = '/kaggle/input/new-tester/f.jpg'\nall_objects = segment_and_filter_objects(image_path, sam_model)\n\n# Display all extracted objects\ndisplay_objects(all_objects, \"All Extracted Objects\")\n\n# Apply additional filtering\nfinal_objects = []\nfor obj in all_objects:\n    gray = cv2.cvtColor(obj, cv2.COLOR_RGB2GRAY)\n    thresh = threshold_otsu(gray)\n    binary = gray > thresh\n    if np.sum(binary) / binary.size > 0.1:  # Ensure object covers at least 10% of the image\n        final_objects.append(obj)\n\n# Display final filtered objects\ndisplay_objects(final_objects, \"Final Filtered Objects\")\n\n# Save final objects\nfor i, obj in enumerate(final_objects):\n    cv2.imwrite(f'object_{i+1}.png', cv2.cvtColor(obj, cv2.COLOR_RGB2BGR))\n\nprint(f\"Extracted {len(all_objects)} initial objects.\")\nprint(f\"Filtered down to {len(final_objects)} final objects.\")\nprint(\"Final objects have been saved as PNG files.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-20T12:11:48.319060Z","iopub.execute_input":"2024-10-20T12:11:48.319572Z","iopub.status.idle":"2024-10-20T12:12:13.671626Z","shell.execute_reply.started":"2024-10-20T12:11:48.319501Z","shell.execute_reply":"2024-10-20T12:12:13.670408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ultralytics import SAM\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nimport pandas as pd\nfrom PIL import Image\nfrom sklearn.metrics import jaccard_score\nfrom scipy.ndimage import gaussian_gradient_magnitude\n\ndef iou(mask1, mask2):\n    return jaccard_score(mask1.flatten(), mask2.flatten())\n\ndef place_on_white_background(image):\n    white_bg = np.ones_like(image) * 255\n    mask = np.any(image != [0, 0, 0], axis=-1)\n    white_bg[mask] = image[mask]\n    return white_bg\n\ndef filter_by_area(mask, min_area, max_area):\n    area = np.sum(mask)\n    return min_area < area < max_area\n\ndef filter_by_contour(mask, min_solidity=0.8, min_aspect_ratio=0.2, max_aspect_ratio=5):\n    contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    if contours:\n        contour = max(contours, key=cv2.contourArea)\n        area = cv2.contourArea(contour)\n        hull = cv2.convexHull(contour)\n        hull_area = cv2.contourArea(hull)\n        solidity = float(area) / hull_area\n        \n        x, y, w, h = cv2.boundingRect(contour)\n        aspect_ratio = float(w) / h if h != 0 else 0\n        \n        return (solidity > min_solidity and \n                min_aspect_ratio < aspect_ratio < max_aspect_ratio)\n    return False\n\ndef is_sharp(image, threshold=100):\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    gradient_magnitude = gaussian_gradient_magnitude(gray, sigma=2)\n    sharpness = np.mean(gradient_magnitude)\n    return sharpness > threshold\n\ndef is_object_like(mask, edge_ratio_threshold=0.1):\n    edges = cv2.Canny(mask.astype(np.uint8), 100, 200)\n    edge_ratio = np.sum(edges > 0) / mask.size\n    return edge_ratio > edge_ratio_threshold\n\ndef process_sam_results(results, model_name, confidence_threshold=0.5, min_area=500, max_area=50000, iou_threshold=0.5):\n    original_image = results[0].orig_img\n    masks = results[0].masks.data.cpu().numpy()\n    boxes = results[0].boxes.data.cpu().numpy()\n    confidences = results[0].boxes.conf.cpu().numpy()\n    \n    unique_objects = []\n    for i, (mask, box, confidence) in enumerate(zip(masks, boxes, confidences)):\n        if confidence < confidence_threshold:\n            continue\n        \n        if not filter_by_area(mask, min_area, max_area):\n            continue\n        \n        if not filter_by_contour(mask):\n            continue\n        \n        if not is_object_like(mask):\n            continue\n        \n        object_mask = mask.astype(bool)\n        object_image = np.zeros_like(original_image)\n        object_image[object_mask] = original_image[object_mask]\n        \n        x1, y1, x2, y2 = map(int, box[:4])\n        cropped_object = object_image[y1:y2, x1:x2]\n        \n        if not is_sharp(cropped_object):\n            continue\n        \n        is_unique = True\n        for existing_obj in unique_objects:\n            if iou(mask, existing_obj['mask']) > iou_threshold:\n                if confidence > existing_obj['confidence']:\n                    unique_objects.remove(existing_obj)\n                else:\n                    is_unique = False\n                break\n        \n        if is_unique:\n            object_on_white = place_on_white_background(cropped_object)\n            \n            object_filename = f\"{model_name}object{len(unique_objects)}.png\"\n            cv2.imwrite(object_filename, cv2.cvtColor(object_on_white, cv2.COLOR_RGB2BGR))\n            \n            unique_objects.append({\n                'model': model_name,\n                'object_id': len(unique_objects),\n                'image': Image.fromarray(object_on_white),\n                'mask': mask,\n                'x1': x1,\n                'y1': y1,\n                'x2': x2,\n                'y2': y2,\n                'filename': object_filename,\n                'confidence': confidence\n            })\n    \n    return unique_objects\n\ndef visualize_objects(objects, cols=5):\n    n = len(objects)\n    rows = (n + cols - 1) // cols\n    fig, axs = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n    axs = axs.flatten()\n    \n    for i, obj in enumerate(objects):\n        axs[i].imshow(obj['image'])\n        axs[i].axis('off')\n        axs[i].set_title(f\"Object {obj['object_id']}\")\n    \n    for i in range(n, len(axs)):\n        axs[i].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Load image\nimg_path = \"/kaggle/input/new-tester/f.jpg\"\nimg = cv2.imread(img_path)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n# Process with SAM2_b model\nsam_model_b = SAM(\"sam2_b.pt\")\nresults_b = sam_model_b(img)\n\n# Display segmentation results\nplt.figure(figsize=(10, 10))\nplt.imshow(results_b[0].plot())\nplt.title(\"Segmentation results - SAM2_b\")\nplt.axis('off')\nplt.show()\n\n# Process and visualize objects with enhanced filtering\nobjects_b = process_sam_results(results_b, \"sam2_b\", confidence_threshold=0.6, min_area=70, max_area=50000, iou_threshold=0.5)\nvisualize_objects(objects_b)\n\n# Create DataFrame\ndf_b = pd.DataFrame([{k: v for k, v in obj.items() if k != 'mask' and k != 'image'} for obj in objects_b])\n\n# Display DataFrame\nprint(df_b)\n\n# Save DataFrame to CSV\ndf_b.to_csv('segmented_objects_sam2_b.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-20T12:21:14.526419Z","iopub.execute_input":"2024-10-20T12:21:14.526898Z","iopub.status.idle":"2024-10-20T12:21:24.367239Z","shell.execute_reply.started":"2024-10-20T12:21:14.526829Z","shell.execute_reply":"2024-10-20T12:21:24.365611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ultralytics import SAM\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nimport pandas as pd\nfrom PIL import Image\nimport os\nfrom sklearn.metrics import jaccard_score\n\ndef iou(mask1, mask2):\n    return jaccard_score(mask1.flatten(), mask2.flatten())\n\ndef place_on_white_background(image):\n    white_bg = np.ones_like(image) * 255\n    mask = np.any(image != [0, 0, 0], axis=-1)\n    white_bg[mask] = image[mask]\n    return white_bg\n\ndef process_sam_results(results, model_name, iou_threshold=0.8, conf_threshold=0.3, min_area=70):\n    original_image = results[0].orig_img\n    masks = results[0].masks.data.cpu().numpy()\n    boxes = results[0].boxes.data.cpu().numpy()\n    \n    unique_objects = []\n    for i, (mask, box) in enumerate(zip(masks, boxes)):\n        if box[4] < conf_threshold:\n            continue\n        \n        area = np.sum(mask)\n        if area < min_area:\n            continue\n        \n        is_unique = True\n        for existing_obj in unique_objects:\n            if iou(mask, existing_obj['mask']) > iou_threshold:\n                is_unique = False\n                break\n        \n        if is_unique:\n            object_mask = mask.astype(bool)\n            object_image = np.zeros_like(original_image)\n            object_image[object_mask] = original_image[object_mask]\n            \n            x1, y1, x2, y2 = map(int, box[:4])\n            cropped_object = object_image[y1:y2, x1:x2]\n            \n            object_on_white = place_on_white_background(cropped_object)\n            \n            object_filename = f\"{model_name}_object{len(unique_objects)}.png\"\n            cv2.imwrite(object_filename, cv2.cvtColor(object_on_white, cv2.COLOR_RGB2BGR))\n            \n            unique_objects.append({\n                'model': model_name,\n                'object_id': len(unique_objects),\n                'image': Image.fromarray(object_on_white),\n                'mask': mask,\n                'x1': x1,\n                'y1': y1,\n                'x2': x2,\n                'y2': y2,\n                'confidence': box[4],\n                'area': area,\n                'filename': object_filename\n            })\n    \n    return unique_objects\n\ndef visualize_objects(objects, cols=5):\n    n = len(objects)\n    rows = (n + cols - 1) // cols\n    fig, axs = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n    axs = axs.flatten()\n    \n    for i, obj in enumerate(objects):\n        axs[i].imshow(obj['image'])\n        axs[i].axis('off')\n        axs[i].set_title(f\"Object {obj['object_id']}\\nConf: {obj['confidence']:.2f}\")\n    \n    for i in range(n, len(axs)):\n        axs[i].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\ndef process_image(img_path, model, conf_threshold=0.3, min_area=100):\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    results = model(img)\n\n    plt.figure(figsize=(10, 10))\n    plt.imshow(results[0].plot())\n    \n    model_name = model.__class__.__name__\n    plt.title(f\"Segmentation results - {model_name}\")\n    plt.axis('off')\n    plt.show()\n\n    objects = process_sam_results(results, model_name, conf_threshold=conf_threshold, min_area=min_area)\n    visualize_objects(objects)\n\n    df = pd.DataFrame([{k: v for k, v in obj.items() if k != 'mask' and k != 'image'} for obj in objects])\n    print(df)\n\n    csv_filename = f'segmented_objects_{model_name}.csv'\n    df.to_csv(csv_filename, index=False)\n    print(f\"Results saved to {csv_filename}\")\n\n    return objects, df\n\n# Load SAM2_b model\nsam_model_b = SAM(\"sam2_b.pt\")\n\n# Process single image\nimg_path = \"/kaggle/input/newest-tester-data/images (1).jpg\"\ntry:\n    objects_b, df_b = process_image(img_path, sam_model_b, conf_threshold=0.3, min_area=100)\nexcept Exception as e:\n    print(f\"An error occurred while processing the image: {e}\")\n\n# If you want to process multiple images in a directory:\n# image_dir = \"/path/to/image/directory\"\n# for img_file in os.listdir(image_dir):\n#     if img_file.endswith(('.jpg', '.png', '.jpeg')):\n#         img_path = os.path.join(image_dir, img_file)\n#         print(f\"Processing {img_file}\")\n#         try:\n#             objects_b, df_b = process_image(img_path, sam_model_b, conf_threshold=0.3, min_area=100)\n#         except Exception as e:\n#             print(f\"An error occurred while processing {img_file}: {e}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-20T12:18:05.237189Z","iopub.execute_input":"2024-10-20T12:18:05.237585Z","iopub.status.idle":"2024-10-20T12:18:16.275324Z","shell.execute_reply.started":"2024-10-20T12:18:05.237546Z","shell.execute_reply":"2024-10-20T12:18:16.274190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom ultralytics import SAM\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nimport pandas as pd\nfrom PIL import Image\nfrom sklearn.metrics import jaccard_score\nfrom torchvision.models import resnet50\nfrom torchvision.transforms import functional as F\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\ndef iou(mask1, mask2):\n    return jaccard_score(mask1.flatten(), mask2.flatten())\n\ndef place_on_white_background(image):\n    white_bg = np.ones_like(image) * 255\n    mask = np.any(image != [0, 0, 0], axis=-1)\n    white_bg[mask] = image[mask]\n    return white_bg\n\ndef process_sam_results(results, sam_model_name, iou_threshold=0.5, min_object_size=50):\n    original_image = results[0].orig_img\n    masks = results[0].masks.data.cpu().numpy()\n    \n    unique_objects = []\n    for i, mask in enumerate(masks):\n        is_unique = True\n        for existing_obj in unique_objects:\n            if iou(mask, existing_obj['mask']) > iou_threshold:\n                is_unique = False\n                break\n        \n        if is_unique:\n            object_mask = mask.astype(bool)\n            object_image = np.zeros_like(original_image)\n            object_image[object_mask] = original_image[object_mask]\n            \n            # Get bounding box\n            y, x = np.where(object_mask)\n            y1, y2, x1, x2 = y.min(), y.max(), x.min(), x.max()\n            \n            cropped_object = object_image[y1:y2, x1:x2]\n            \n            # Skip extremely small objects\n            if cropped_object.shape[0] < min_object_size or cropped_object.shape[1] < min_object_size:\n                continue\n            \n            object_on_white = place_on_white_background(cropped_object)\n            \n            object_filename = f\"{sam_model_name}object{len(unique_objects)}.png\"\n            cv2.imwrite(object_filename, cv2.cvtColor(object_on_white, cv2.COLOR_RGB2BGR))\n            \n            unique_objects.append({\n                'model': sam_model_name,\n                'object_id': len(unique_objects),\n                'image': Image.fromarray(object_on_white),\n                'mask': mask,\n                'bbox': [x1, y1, x2, y2],\n                'filename': object_filename\n            })\n    \n    return unique_objects\n\ndef visualize_objects(objects, cols=5):\n    n = len(objects)\n    rows = (n + cols - 1) // cols\n    fig, axs = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n    axs = axs.flatten()\n    \n    for i, obj in enumerate(objects):\n        axs[i].imshow(obj['image'])\n        axs[i].axis('off')\n        axs[i].set_title(f\"Object {obj['object_id']}\")\n    \n    for i in range(n, len(axs)):\n        axs[i].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\ndef is_object(image, model):\n    # Preprocess the image\n    image = F.to_tensor(image)\n    image = F.normalize(image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    image = image.unsqueeze(0).to(device)\n    \n    # Get model prediction\n    with torch.no_grad():\n        output = model(image)\n    \n    # Check if the top predicted class is not background (assuming background is class 0)\n    _, predicted = output.max(1)\n    return predicted.item() != 0\n\n# Load SAM model\nsam_model = SAM(\"sam2_b.pt\").to(device)\n\n# Load pre-trained ResNet model for object classification\n\n\n# Load image\nimg_path = \"/kaggle/input/newest-tester-data/images (1).jpg\"\nimg = cv2.imread(img_path)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n# Display original image\nplt.figure(figsize=(10, 10))\nplt.imshow(img)\nplt.title(\"Original Image\")\nplt.axis('off')\nplt.show()\n\n# Process with SAM model\nresults = sam_model(img)\n\n# Display segmentation results\nplt.figure(figsize=(10, 10))\nplt.imshow(results[0].plot())\nplt.title(\"Segmentation results - SAM\")\nplt.axis('off')\nplt.show()\n\n# Process SAM results\nobjects = process_sam_results(results, \"sam\")\n\nprint(f\"Number of objects detected by SAM: {len(objects)}\")\n\n# Visualize objects from SAM\nvisualize_objects(objects)\n\n# Filter objects using ResNet\n\n# Visualize filtered objects\n\n# Create DataFrame\ndf = pd.DataFrame([{k: v for k, v in obj.items() if k not in ['mask', 'image']} for obj in objects])\n\n# Display DataFrame\nprint(df)\n# Save DataFrame to CSV\ndf.to_csv('segmented_objects.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-20T12:46:10.153590Z","iopub.execute_input":"2024-10-20T12:46:10.154069Z","iopub.status.idle":"2024-10-20T12:46:21.056623Z","shell.execute_reply.started":"2024-10-20T12:46:10.154019Z","shell.execute_reply":"2024-10-20T12:46:21.055477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch,gc\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-10-20T12:45:11.305386Z","iopub.execute_input":"2024-10-20T12:45:11.305740Z","iopub.status.idle":"2024-10-20T12:45:11.817390Z","shell.execute_reply.started":"2024-10-20T12:45:11.305700Z","shell.execute_reply":"2024-10-20T12:45:11.816247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install ultralytics","metadata":{"execution":{"iopub.status.busy":"2024-10-20T14:46:23.509945Z","iopub.execute_input":"2024-10-20T14:46:23.510310Z","iopub.status.idle":"2024-10-20T14:46:37.233695Z","shell.execute_reply.started":"2024-10-20T14:46:23.510271Z","shell.execute_reply":"2024-10-20T14:46:37.232680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install ultralytics","metadata":{"execution":{"iopub.status.busy":"2024-10-20T16:51:12.519477Z","iopub.execute_input":"2024-10-20T16:51:12.519880Z","iopub.status.idle":"2024-10-20T16:51:27.248536Z","shell.execute_reply.started":"2024-10-20T16:51:12.519840Z","shell.execute_reply":"2024-10-20T16:51:27.247496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms, models\nfrom ultralytics import SAM\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nimport pandas as pd\nfrom PIL import Image\nimport os\nfrom sklearn.metrics import jaccard_score\nfrom transformers import Qwen2VLForConditionalGeneration, AutoProcessor\nimport warnings\nimport re\nfrom datetime import datetime\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Ensure GPU usage if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Helper functions\ndef iou(mask1, mask2):\n    return jaccard_score(mask1.flatten(), mask2.flatten())\n\ndef place_on_white_background(image):\n    white_bg = np.ones_like(image) * 255\n    mask = np.any(image != [0, 0, 0], axis=-1)\n    white_bg[mask] = image[mask]\n    return white_bg\n\ndef process_sam_results(results, model_name, iou_threshold=0.8, min_object_size=50):\n    original_image = results[0].orig_img\n    masks = results[0].masks.data.cpu().numpy()\n    \n    unique_objects = []\n    for i, mask in enumerate(masks):\n        is_unique = True\n        for existing_obj in unique_objects:\n            if iou(mask, existing_obj['mask']) > iou_threshold:\n                is_unique = False\n                break\n        \n        if is_unique:\n            object_mask = mask.astype(bool)\n            object_image = np.zeros_like(original_image)\n            object_image[object_mask] = original_image[object_mask]\n            \n            y, x = np.where(object_mask)\n            y1, y2, x1, x2 = y.min(), y.max(), x.min(), x.max()\n            \n            cropped_object = object_image[y1:y2, x1:x2]\n            \n            if cropped_object.shape[0] < min_object_size or cropped_object.shape[1] < min_object_size:\n                continue\n            \n            object_on_white = place_on_white_background(cropped_object)\n            \n            object_filename = f\"{model_name}object{len(unique_objects)}.png\"\n            cv2.imwrite(object_filename, cv2.cvtColor(object_on_white, cv2.COLOR_RGB2BGR))\n            \n            unique_objects.append({\n                'model': model_name,\n                'object_id': len(unique_objects),\n                'image': Image.fromarray(object_on_white),\n                'mask': mask,\n                'bbox': [x1, y1, x2, y2],\n                'filename': object_filename\n            })\n    \n    return unique_objects\n\ndef visualize_objects(objects, cols=5):\n    n = len(objects)\n    rows = (n + cols - 1) // cols\n    fig, axs = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n    axs = axs.flatten()\n    \n    for i, obj in enumerate(objects):\n        axs[i].imshow(obj['image'])\n        axs[i].axis('off')\n        axs[i].set_title(f\"Object {obj['object_id']}\")\n    \n    for i in range(n, len(axs)):\n        axs[i].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Load models\nsam_model = SAM(\"sam2_b.pt\")\n\npacked_unpacked_model = models.efficientnet_b0(pretrained=False)\npacked_unpacked_model.classifier[1] = nn.Linear(packed_unpacked_model.classifier[1].in_features, 2)\npacked_unpacked_model.load_state_dict(torch.load('/kaggle/input/models-loaded/keras/default/1/efficientnet_b0_packed_unpacked (2).pth', map_location=device))\npacked_unpacked_model.to(device)\npacked_unpacked_model.eval()\n\nfruit_veg_model = models.efficientnet_b0(pretrained=False)\nfruit_veg_model.classifier[1] = nn.Linear(fruit_veg_model.classifier[1].in_features, 54)\nfruit_veg_model.load_state_dict(torch.load('/kaggle/input/models-loaded/keras/default/1/efficientnet_b0_fruit_veg_1 (1).pth', map_location=device))\nfruit_veg_model.to(device)\nfruit_veg_model.eval()\n\nfresh_rotten_model = models.efficientnet_b0(pretrained=False)\nfresh_rotten_model.classifier[1] = nn.Linear(fresh_rotten_model.classifier[1].in_features, 2)\nfresh_rotten_model.load_state_dict(torch.load('/kaggle/input/models-loaded/keras/default/1/efficientnet_b0_fruit_veg (2).pth', map_location=device))\nfresh_rotten_model.to(device)\nfresh_rotten_model.eval()\n\nqwen_model = Qwen2VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2-VL-2B-Instruct\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\",\n)\nqwen_processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n\n# Define categories and transformations\npacked_unpacked_categories = ['Packed', 'Unpacked']\npacked_unpacked_idx_to_category = {idx: category for idx, category in enumerate(packed_unpacked_categories)}\n\nfruit_veg_categories = [\n    'Orange', 'Tamarillo', 'Lime', 'Pomegranate', 'Plum', 'Pineapple', 'Apple', 'Dates', 'Papaya', 'Guava',\n    'Beetroot', 'Pear', 'Strawberry', 'Blueberry', 'Lulo', 'Avacado', 'Lemon', 'Kaki', 'Peach', 'Grape',\n    'Banana', 'Cherry', 'Watermelon', 'Mango', 'Grapefruit', 'Broccoli', 'Capsicum', 'Radish', 'Tomato', 'Turnip',\n    'Ginger', 'Zucchini', 'Brinjal', 'Pumpkin', 'Bell Pepper', 'Carrot', 'New Mexico Green Chile', 'Eggplant',\n    'Baby Corn', 'Zucchini dark', 'Sweet corn', 'Cabbage', 'Bitter_Gourd', 'Cauliflower', 'Chile Pepper',\n    'Sweet Potato', 'Bean', 'Cucumber', 'Bottle Gourd', 'Garlic', 'Peas', 'Onion', 'Potato', 'Spinach'\n]\nfruit_veg_idx_to_category = {idx: category for idx, category in enumerate(fruit_veg_categories)}\n\nfresh_rotten_categories = ['Fresh', 'Rotten']\nfresh_rotten_idx_to_category = {idx: category for idx, category in enumerate(fresh_rotten_categories)}\n\npacked_categories = [\n    'Staples', 'Snacks & Beverages', 'Packaged Food', 'Personal & Baby Care',\n    'Household Care', 'Dairy & Eggs', 'Home & Kitchen'\n]\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ndef predict_image_class(model, image_path, idx_to_category):\n    try:\n        image = Image.open(image_path).convert('RGB')\n        image = transform(image).unsqueeze(0).to(device)\n\n        with torch.no_grad():\n            outputs = model(image)\n            _, predicted_label = torch.max(outputs, 1)\n\n        predicted_label = predicted_label.item()\n        predicted_category = idx_to_category[predicted_label]\n\n        return predicted_label, predicted_category\n    except Exception as e:\n        print(f\"Error predicting class for {image_path}: {str(e)}\")\n        return None, None\n\ndef get_product_info(image_path, question):\n    try:\n        image = Image.open(image_path)\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"image\"},\n                    {\"type\": \"text\", \"text\": question}\n                ]\n            }\n        ]\n        text_prompt = qwen_processor.apply_chat_template(messages, add_generation_prompt=True)\n        inputs = qwen_processor(text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\")\n        inputs = inputs.to(\"cuda\")\n        output_ids = qwen_model.generate(**inputs, max_new_tokens=1024)\n        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n        output_text = qwen_processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n        return output_text[0]\n    except Exception as e:\n        print(f\"Error getting product info for {image_path}: {str(e)}\")\n        return \"Error: Unable to process image\"\n\ndef is_valid_date(date_str):\n    date_formats = [\n        \"%d/%m/%Y\", \"%d-%m-%Y\", \"%Y/%m/%d\", \"%Y-%m-%d\", \n        \"%m/%d/%Y\", \"%m-%d-%Y\", \"%d %b %Y\", \"%d %B %Y\", \n        \"%d/%m/%y\", \"%d-%m-%y\", \"%Y%m%d\"\n    ]\n    for date_format in date_formats:\n        try:\n            datetime.strptime(date_str, date_format)\n            return True\n        except ValueError:\n            continue\n    return False\n\ndef process_single_object(image_path):\n    # Check if packed or unpacked\n    _, packed_unpacked_category = predict_image_class(packed_unpacked_model, image_path, packed_unpacked_idx_to_category)\n    \n    if packed_unpacked_category == 'Packed':\n        # Get product information using Qwen model\n        product_name = get_product_info(image_path, \"What is the name of the product? NOTE: JUST PROVIDE NAME AS THE ANSWER\")\n        expiry_date = get_product_info(image_path, \"What is the expiry date of the product? If not visible, say 'Not visible'\")\n        description = get_product_info(image_path, \"Provide a brief description of the product\")\n        category_info = get_product_info(image_path, \"Classify the product into one of these categories: Staples, Snacks & Beverages, Packaged Food, Personal & Baby Care, Household Care, Dairy & Eggs, Home & Kitchen\")\n        \n        return pd.DataFrame({\n            'name': [product_name],\n            'expiry_date': [expiry_date],\n            'description': [description],\n            'category': [category_info],\n            'type': ['Packed'],\n            'frequency': [1]\n        })\n    else:\n        # Classify fruit/vegetable\n        _, fruit_veg_category = predict_image_class(fruit_veg_model, image_path, fruit_veg_idx_to_category)\n        \n        # Determine if fresh or rotten\n        _, fresh_rotten_category = predict_image_class(fresh_rotten_model, image_path, fresh_rotten_idx_to_category)\n        \n        return pd.DataFrame({\n            'name': [fruit_veg_category],\n            'condition': [fresh_rotten_category],\n            'type': ['Unpacked'],\n            'frequency': [1],\n            'indepth_condition': ['To be determined'],  # Placeholder\n            'weight': ['To be determined']  # Placeholder\n        })\n\ndef process_multiple_objects(image_path, is_front):\n    # Perform segmentation\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    results = sam_model(img)\n    objects = process_sam_results(results, \"sam\")\n    \n    df_list = []\n    for obj in objects:\n        obj_image_path = obj['filename']\n        obj_df = process_single_object(obj_image_path)\n        obj_df['bbox'] = [obj['bbox']]\n        obj_df['side'] = ['Front' if is_front else 'Back']\n        df_list.append(obj_df)\n    \n    return pd.concat(df_list, ignore_index=True)\n\ndef main():\n    image_type = input(\"Is this a single object image or multiple objects image? (single/multiple): \").lower()\n    \n    if image_type == 'single':\n        front_image_path = input(\"Enter the path to the front image: \")\n        back_image_path = input(\"Enter the path to the back image: \")\n        \n        front_df = process_single_object(front_image_path)\n        front_df['side'] = 'Front'\n        back_df = process_single_object(back_image_path)\n        back_df['side'] = 'Back'\n        \n        combined_df = pd.concat([front_df, back_df], ignore_index=True)\n        \n    elif image_type == 'multiple':\n        front_image_path = input(\"Enter the path to the front image: \")\n        back_image_path = input(\"Enter the path to the back image: \")\n        \n        front_df = process_multiple_objects(front_image_path, is_front=True)\n        back_df = process_multiple_objects(back_image_path, is_front=False)\n        \n        combined_df = pd.concat([front_df, back_df], ignore_index=True)\n        \n        # Instead of grouping by 'bbox', we'll create a unique identifier\n        combined_df['object_id'] = range(len(combined_df))\n        \n        # Merge front and back information based on object overlap\n        # This is a simplified approach and may need refinement\n        merged_df = combined_df.groupby('object_id').first().reset_index()\n        \n    else:\n        print(\"Invalid input. Please choose 'single' or 'multiple'.\")\n        return\n    \n    # Create separate DataFrames for packed and unpacked items\n    packed_df = combined_df[combined_df['type'] == 'Packed']\n    unpacked_df = combined_df[combined_df['type'] == 'Unpacked']\n    \n    # Save results\n    combined_df.to_csv('combined_items.csv', index=False)\n    packed_df.to_csv('packed_items.csv', index=False)\n    unpacked_df.to_csv('unpacked_items.csv', index=False)\n    \n    print(\"Processing complete. Results saved to CSV files.\")\n    print(\"\\nCombined items:\")\n    print(combined_df)\n    print(\"\\nPacked items:\")\n    print(packed_df)\n    print(\"\\nUnpacked items:\")\n    print(unpacked_df)\n\n    \nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-10-20T16:56:14.949533Z","iopub.execute_input":"2024-10-20T16:56:14.950270Z","iopub.status.idle":"2024-10-20T16:56:51.727554Z","shell.execute_reply.started":"2024-10-20T16:56:14.950227Z","shell.execute_reply":"2024-10-20T16:56:51.726493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T16:55:50.024481Z","iopub.execute_input":"2024-10-20T16:55:50.025004Z","iopub.status.idle":"2024-10-20T16:55:50.044426Z","shell.execute_reply.started":"2024-10-20T16:55:50.024952Z","shell.execute_reply":"2024-10-20T16:55:50.043424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T16:55:50.045552Z","iopub.execute_input":"2024-10-20T16:55:50.045981Z","iopub.status.idle":"2024-10-20T16:55:50.063152Z","shell.execute_reply.started":"2024-10-20T16:55:50.045948Z","shell.execute_reply":"2024-10-20T16:55:50.062114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-10-20T16:55:50.064484Z","iopub.execute_input":"2024-10-20T16:55:50.064878Z","iopub.status.idle":"2024-10-20T16:55:50.082967Z","shell.execute_reply.started":"2024-10-20T16:55:50.064841Z","shell.execute_reply":"2024-10-20T16:55:50.082146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-10-20T16:54:57.791002Z","iopub.execute_input":"2024-10-20T16:54:57.791321Z","iopub.status.idle":"2024-10-20T16:55:50.021545Z","shell.execute_reply.started":"2024-10-20T16:54:57.791285Z","shell.execute_reply":"2024-10-20T16:55:50.020435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-10-20T16:04:26.154096Z","iopub.execute_input":"2024-10-20T16:04:26.154792Z","iopub.status.idle":"2024-10-20T16:04:32.506961Z","shell.execute_reply.started":"2024-10-20T16:04:26.154743Z","shell.execute_reply":"2024-10-20T16:04:32.506090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}